{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Projection\n",
        "\n",
        "First map Google drive."
      ],
      "metadata": {
        "id": "eDMHtf7ccifv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJPIlxl8hAg5",
        "outputId": "0706a391-2207-4274-a5d4-a75e3d2994ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Note: using Google CoLab\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    COLAB = True\n",
        "    print(\"Note: using Google CoLab\")\n",
        "except:\n",
        "    print(\"Note: not using Google CoLab\")\n",
        "    COLAB = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the Glove Embeddings"
      ],
      "metadata": {
        "id": "2cvkEW0rIZjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -c \"https://nlp.stanford.edu/data/glove.6B.zip\"\n",
        "!unzip /content/glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdE224AVJHxa",
        "outputId": "6a5acb5f-9e45-40f7-9cac-8047c64b1295"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-05 04:10:28--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2022-10-05 04:10:29--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  4.23MB/s    in 3m 50s  \n",
            "\n",
            "2022-10-05 04:14:20 (3.57 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  /content/glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "glove_file = '/content/glove.6B.300d.txt'\n",
        "tmp_file = get_tmpfile(\"test_word2vec.txt\")\n",
        "_ = glove2word2vec(glove_file, tmp_file)\n",
        "w2vec_model = KeyedVectors.load_word2vec_format(tmp_file)"
      ],
      "metadata": {
        "id": "bhBfLkU-Nt3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NLP Demo"
      ],
      "metadata": {
        "id": "Uwgl1rkuQaRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w2vec_model.most_similar(positive=['woman', 'king'], negative=['man'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "immf5pMjQdRC",
        "outputId": "0f788c1f-239c-454e-a03b-7f9cf834b6a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('queen', 0.6713277101516724),\n",
              " ('princess', 0.5432624220848083),\n",
              " ('throne', 0.5386104583740234),\n",
              " ('monarch', 0.5347574949264526),\n",
              " ('daughter', 0.498025119304657),\n",
              " ('mother', 0.4956442713737488),\n",
              " ('elizabeth', 0.4832652509212494),\n",
              " ('kingdom', 0.47747087478637695),\n",
              " ('prince', 0.4668239951133728),\n",
              " ('wife', 0.4647327661514282)]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2vec_model['dog'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJPkTG-WRCw9",
        "outputId": "eeef2545-7181-4824-903f-87bc7c06f51b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300,)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Perform NLP Prediction\n",
        "Load the three data files."
      ],
      "metadata": {
        "id": "yN7476ZB3MTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "PATH = \"/content/drive/MyDrive/projects/demand/\"\n",
        "\n",
        "df_sales = pd.read_csv(\"https://data.heatonresearch.com/wustl/CABI/demand-forecast/sales_train.csv\", parse_dates=['date'])\n",
        "df_items = pd.read_csv(\"https://data.heatonresearch.com/wustl/CABI/demand-forecast/items.csv\")\n",
        "df_resturant = pd.read_csv(\"https://data.heatonresearch.com/wustl/CABI/demand-forecast/resturants.csv\")"
      ],
      "metadata": {
        "id": "5WEfyt5ShSeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_title(model, name):\n",
        "  v = None\n",
        "  i = 0\n",
        "  for word in name.split(' '):\n",
        "    if word == 'vegi': word = \"vegetable\"\n",
        "    if word == 'smoothy': word = \"malt\"\n",
        "    i+=1\n",
        "    if v is None:\n",
        "      v=model[word].copy()\n",
        "    else:\n",
        "      v+=model[word]\n",
        "  v/=i\n",
        "  return v\n",
        "\n",
        "item_lookup = {}\n",
        "for i, name in zip(list(df_items.id),list(df_items.name)):\n",
        "  v = process_title(w2vec_model,name)\n",
        "  item_lookup[i] = v\n",
        "\n",
        "#r = process_title(model, 'breaded fish with vegetables meal')\n",
        "print(len(item_lookup))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bS1m-S85RPp2",
        "outputId": "f45f695a-4418-4592-abc3-b0ca8e3f30c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utility function to create sequences."
      ],
      "metadata": {
        "id": "__jGovBB3QUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def series_to_supervised(data, window=1, lag=1, dropnan=True):\n",
        "    cols, names = list(), list()\n",
        "    # Input sequence (t-n, ... t-1)\n",
        "    for i in range(window, 0, -1):\n",
        "        cols.append(data.shift(i))\n",
        "        names += [('%s(t-%d)' % (col, i)) for col in data.columns]\n",
        "    # Current timestep (t=0)\n",
        "    cols.append(data)\n",
        "    names += [('%s(t)' % (col)) for col in data.columns]\n",
        "    # Target timestep (t=lag)\n",
        "    cols.append(data.shift(-lag))\n",
        "    names += [('%s(t+%d)' % (col, lag)) for col in data.columns]\n",
        "    # Put it all together\n",
        "    agg = pd.concat(cols, axis=1)\n",
        "    agg.columns = names\n",
        "    # Drop rows with NaN values\n",
        "    if dropnan:\n",
        "        agg.dropna(inplace=True)\n",
        "    return agg"
      ],
      "metadata": {
        "id": "Kz2MmH3Cloi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Join the items and sales tables so that we can look up the store id for each item."
      ],
      "metadata": {
        "id": "DmL154wv3YIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_items2 = df_items[['id','store_id']]\n",
        "df_train = df_sales.merge(df_items2,left_on='item_id',right_on='id')\n",
        "df_train[['date','item_id','item_count','store_id']]\n",
        "\n",
        "df_train = df_train.sort_values('date').groupby(['item_id', 'store_id', 'date'], as_index=False)\n",
        "df_train = df_train.agg({'item_count':['mean']})\n",
        "df_train.columns = ['item', 'store', 'date', 'sales']\n",
        "df_train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "3S7LIIqG3ihU",
        "outputId": "a1524568-9b20-478d-e3ee-584908139c44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   item  store       date  sales\n",
              "0     1      4 2020-12-01    1.0\n",
              "1     1      4 2021-10-14    1.0\n",
              "2     2      4 2020-04-30    1.0\n",
              "3     2      4 2020-06-09    1.0\n",
              "4     2      4 2020-12-26    1.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4c7e9543-e0fe-4900-96d9-6925ec6658e4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>item</th>\n",
              "      <th>store</th>\n",
              "      <th>date</th>\n",
              "      <th>sales</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2020-12-01</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2021-10-14</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2020-04-30</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2020-06-09</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2020-12-26</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4c7e9543-e0fe-4900-96d9-6925ec6658e4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4c7e9543-e0fe-4900-96d9-6925ec6658e4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4c7e9543-e0fe-4900-96d9-6925ec6658e4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['dow'] = df_train['date'].dt.dayofweek\n",
        "df_train['doy'] = df_train['date'].dt.dayofyear\n",
        "\n",
        "df_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "juImdYgR-rNi",
        "outputId": "b4952149-2c0b-4170-bcc1-bb6f549cc073"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       item  store       date  sales  dow  doy\n",
              "0         1      4 2020-12-01    1.0    1  336\n",
              "1         1      4 2021-10-14    1.0    3  287\n",
              "2         2      4 2020-04-30    1.0    3  121\n",
              "3         2      4 2020-06-09    1.0    1  161\n",
              "4         2      4 2020-12-26    1.0    5  361\n",
              "...     ...    ...        ...    ...  ...  ...\n",
              "15636   100      2 2021-08-10    1.0    1  222\n",
              "15637   100      2 2021-11-08    1.0    0  312\n",
              "15638   100      2 2021-11-20    1.0    5  324\n",
              "15639   100      2 2021-12-03    1.0    4  337\n",
              "15640   100      2 2021-12-11    1.0    5  345\n",
              "\n",
              "[15641 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6ba4e4f4-3dd7-418e-9ab2-e8334854eaba\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>item</th>\n",
              "      <th>store</th>\n",
              "      <th>date</th>\n",
              "      <th>sales</th>\n",
              "      <th>dow</th>\n",
              "      <th>doy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2020-12-01</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2021-10-14</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3</td>\n",
              "      <td>287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2020-04-30</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3</td>\n",
              "      <td>121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2020-06-09</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2020-12-26</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5</td>\n",
              "      <td>361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15636</th>\n",
              "      <td>100</td>\n",
              "      <td>2</td>\n",
              "      <td>2021-08-10</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15637</th>\n",
              "      <td>100</td>\n",
              "      <td>2</td>\n",
              "      <td>2021-11-08</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15638</th>\n",
              "      <td>100</td>\n",
              "      <td>2</td>\n",
              "      <td>2021-11-20</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5</td>\n",
              "      <td>324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15639</th>\n",
              "      <td>100</td>\n",
              "      <td>2</td>\n",
              "      <td>2021-12-03</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4</td>\n",
              "      <td>337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15640</th>\n",
              "      <td>100</td>\n",
              "      <td>2</td>\n",
              "      <td>2021-12-11</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5</td>\n",
              "      <td>345</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15641 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6ba4e4f4-3dd7-418e-9ab2-e8334854eaba')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6ba4e4f4-3dd7-418e-9ab2-e8334854eaba button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6ba4e4f4-3dd7-418e-9ab2-e8334854eaba');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build the sequence data."
      ],
      "metadata": {
        "id": "DSslf7_x3f_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "window = 29\n",
        "future_span = 30\n",
        "series = series_to_supervised(df_train.drop('date', axis=1), window=window, lag=future_span)\n",
        "series.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "ZKf1jppTlvi4",
        "outputId": "b1368ba9-ee8c-4e3c-8880-dbeb5fbc2bbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    item(t-29)  store(t-29)  sales(t-29)  dow(t-29)  doy(t-29)  item(t-28)  \\\n",
              "29         1.0          4.0          1.0        1.0      336.0         1.0   \n",
              "30         1.0          4.0          1.0        3.0      287.0         2.0   \n",
              "31         2.0          4.0          1.0        3.0      121.0         2.0   \n",
              "32         2.0          4.0          1.0        1.0      161.0         2.0   \n",
              "33         2.0          4.0          1.0        5.0      361.0         2.0   \n",
              "\n",
              "    store(t-28)  sales(t-28)  dow(t-28)  doy(t-28)  ...  item(t)  store(t)  \\\n",
              "29          4.0          1.0        3.0      287.0  ...        3         1   \n",
              "30          4.0          1.0        3.0      121.0  ...        3         1   \n",
              "31          4.0          1.0        1.0      161.0  ...        3         1   \n",
              "32          4.0          1.0        5.0      361.0  ...        3         1   \n",
              "33          4.0          1.0        4.0      218.0  ...        3         1   \n",
              "\n",
              "    sales(t)  dow(t)  doy(t)  item(t+30)  store(t+30)  sales(t+30)  dow(t+30)  \\\n",
              "29       3.0       5      32         3.0          1.0          3.0        3.0   \n",
              "30       1.0       6      33         3.0          1.0          1.0        4.0   \n",
              "31       1.0       0      34         3.0          1.0          2.0        5.0   \n",
              "32       2.0       2      36         3.0          1.0          1.0        6.0   \n",
              "33       1.0       3      37         3.0          1.0          1.0        0.0   \n",
              "\n",
              "    doy(t+30)  \n",
              "29       65.0  \n",
              "30       66.0  \n",
              "31       67.0  \n",
              "32       68.0  \n",
              "33       69.0  \n",
              "\n",
              "[5 rows x 155 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-413b1d12-5c4a-4a19-8e56-00b2d6d31bce\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>item(t-29)</th>\n",
              "      <th>store(t-29)</th>\n",
              "      <th>sales(t-29)</th>\n",
              "      <th>dow(t-29)</th>\n",
              "      <th>doy(t-29)</th>\n",
              "      <th>item(t-28)</th>\n",
              "      <th>store(t-28)</th>\n",
              "      <th>sales(t-28)</th>\n",
              "      <th>dow(t-28)</th>\n",
              "      <th>doy(t-28)</th>\n",
              "      <th>...</th>\n",
              "      <th>item(t)</th>\n",
              "      <th>store(t)</th>\n",
              "      <th>sales(t)</th>\n",
              "      <th>dow(t)</th>\n",
              "      <th>doy(t)</th>\n",
              "      <th>item(t+30)</th>\n",
              "      <th>store(t+30)</th>\n",
              "      <th>sales(t+30)</th>\n",
              "      <th>dow(t+30)</th>\n",
              "      <th>doy(t+30)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>336.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>287.0</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5</td>\n",
              "      <td>32</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>65.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>287.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>33</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>66.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>161.0</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>34</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>67.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>161.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>361.0</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2</td>\n",
              "      <td>36</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>68.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>361.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>218.0</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3</td>\n",
              "      <td>37</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>69.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 155 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-413b1d12-5c4a-4a19-8e56-00b2d6d31bce')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-413b1d12-5c4a-4a19-8e56-00b2d6d31bce button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-413b1d12-5c4a-4a19-8e56-00b2d6d31bce');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove sequences that did not have enough data."
      ],
      "metadata": {
        "id": "gMBAkJvR3nce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove edge cases, where there were not enough values to complete a series\n",
        "last_item = 'item(t-%d)' % window\n",
        "last_store = 'store(t-%d)' % window\n",
        "last_dow = 'dow(t-%d)' % window\n",
        "last_doy = 'doy(t-%d)' % window\n",
        "series = series[(series['store(t)'] == series[last_store])]\n",
        "series = series[(series['item(t)'] == series[last_item])]\n",
        "#series = series[(series['dow(t)'] == series[last_dow])]\n",
        "#series = series[(series['doy(t)'] == series[last_doy])]"
      ],
      "metadata": {
        "id": "_6D8BDtu-DWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will predict with sales, and our engineered features."
      ],
      "metadata": {
        "id": "4quQLgwg3xfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.utils.np_utils import to_categorical   \n",
        "\n",
        "def drop_column(df, col):\n",
        "  columns_to_drop = [('%s(t+%d)' % (col, future_span))]\n",
        "  for i in range(window, 0, -1):\n",
        "      columns_to_drop += [('%s(t-%d)' % (col, i))]\n",
        "  df.drop(columns_to_drop, axis=1, inplace=True, errors='ignore')\n",
        "  df.drop([f\"{col}(t)\"], axis=1, inplace=True, errors='ignore')\n",
        "\n",
        "def cat_seq(df, col):\n",
        "  return to_categorical(df[col].values)\n",
        "\n",
        "# Label\n",
        "labels_col = 'sales(t+%d)' % future_span\n",
        "labels = series[labels_col]\n",
        "series.drop(labels_col, axis=1, inplace=True)\n",
        "series.drop('item(t+%d)' % future_span, axis=1, inplace=True)\n",
        "series.drop('store(t+%d)' % future_span, axis=1, inplace=True)\n",
        "series.drop('dow(t+%d)' % future_span, axis=1, inplace=True)\n",
        "series.drop('doy(t+%d)' % future_span, axis=1, inplace=True)\n",
        "\n",
        "# Get sales sequences\n",
        "series2 = series.copy()\n",
        "drop_column(series2, \"item\")\n",
        "drop_column(series2, \"store\")\n",
        "drop_column(series2, \"dow\")\n",
        "drop_column(series2, \"doy\")\n",
        "sales_series = series2.values\n",
        "\n",
        "# Day of week as a number\n",
        "series2 = series.copy()\n",
        "drop_column(series2, \"item\")\n",
        "drop_column(series2, \"store\")\n",
        "drop_column(series2, \"doy\")\n",
        "drop_column(series2, \"sales\")\n",
        "dow_series = series2.values\n",
        "\n",
        "# Get day of year sequences\n",
        "series2 = series.copy()\n",
        "drop_column(series2, \"item\")\n",
        "drop_column(series2, \"store\")\n",
        "drop_column(series2, \"dow\")\n",
        "drop_column(series2, \"sales\")\n",
        "doy_series = series2.values\n",
        "\n",
        "# Day of year\n",
        "t1 = sales_series.reshape(sales_series.shape + (1,))\n",
        "t2 = dow_series.reshape(dow_series.shape + (1,)) \n",
        "t3 = doy_series.reshape(doy_series.shape + (1,))\n",
        "\n",
        "# Create predictors (x)\n",
        "vec_size = w2vec_model['test'].shape[0]\n",
        "\n",
        "lst = []\n",
        "for item in list(series['item(t-1)']):\n",
        "  lst.append(item_lookup[item])\n",
        "\n",
        "x1 = np.concatenate([t1,t2,t3],axis=2)\n",
        "x2 = np.concatenate(lst).reshape((series.shape[0],vec_size))\n",
        "\n",
        "x = [x1,x2]"
      ],
      "metadata": {
        "id": "RRhDirsoZ4dZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(t1.shape)\n",
        "print(t2.shape)\n",
        "print(t3.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0gn1hXDk4jY",
        "outputId": "4826b971-f047-4e90-eef4-fea7ed331270"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(13813, 30, 1)\n",
            "(13813, 30, 1)\n",
            "(13813, 30, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#store_series = to_categorical(series['store(t)'].values)\n",
        "#store_series.shape"
      ],
      "metadata": {
        "id": "3mrMep9homxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract the predictors (x sequences) and the label (future prediction)"
      ],
      "metadata": {
        "id": "tLrgz7RK33f1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_SIZE = 0.4\n",
        "\n",
        "mask = np.random.random(size=x[0].shape[0])<TEST_SIZE\n",
        "\n",
        "X_train = []\n",
        "X_valid = []\n",
        "\n",
        "for subx in x:\n",
        "  X_train.append(subx[~mask])\n",
        "  X_valid.append(subx[mask])\n",
        "\n",
        "Y_train = labels.values[~mask]\n",
        "Y_valid = labels.values[mask]\n",
        "\n",
        "print('Train set shape x1:', X_train[0].shape)\n",
        "print('Train set shape x2:', X_train[1].shape)\n",
        "print('Validation set shape x1:', X_valid[0].shape)\n",
        "print('Validation set shape x2:', X_valid[1].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntMujMSFj4v_",
        "outputId": "0464c2ce-08f4-4079-97b7-fcd9c3963032"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set shape x1: (8305, 30, 3)\n",
            "Train set shape x2: (8305, 300)\n",
            "Validation set shape x1: (5508, 30, 3)\n",
            "Validation set shape x2: (5508, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construct the neural network."
      ],
      "metadata": {
        "id": "IazoyOgL4D5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf \n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
        "from keras.layers import Dense, LSTM, RepeatVector, TimeDistributed, Flatten, Dropout, concatenate, Input\n",
        "import keras\n",
        "\n",
        "epochs = 500\n",
        "batch = 256\n",
        "lr = 0.0003\n",
        "adam = tf.keras.optimizers.Adam(lr)\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "A1 = Input(shape=(X_train[0].shape[1], X_train[0].shape[2]),name='A1')\n",
        "A2 = Conv1D(filters=64, kernel_size=8, activation='relu')(A1)\n",
        "A3 = MaxPooling1D(pool_size=4)(A2)\n",
        "A4 = Flatten()(A3)\n",
        "A5 = Dense(50, activation='relu')(A4)\n",
        "A6 = Dropout(0.2)(A5)\n",
        "\n",
        "B1 = Input(shape=X_train[1].shape[1],name='B1')\n",
        "B2 = Dense(16, activation='relu',name='B2')(B1)\n",
        "\n",
        "M1 = concatenate([A6,B2])\n",
        "M2 = Dense(1,name='M2')(M1)\n",
        "\n",
        "model = Model(inputs=[A1, B1],outputs=[M2])\n",
        "model.compile(loss='mse', optimizer=adam)\n",
        "model.summary()\n",
        "\n",
        "#model = Sequential()\n",
        "#model.add(LSTM(50, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "#model.add(Dense(1))\n",
        "#model.compile(loss='mse', optimizer=adam)\n",
        "#model.summary()"
      ],
      "metadata": {
        "id": "iSX-MNvjz81I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42e073c7-b153-4681-bc2c-24f70953f625"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " A1 (InputLayer)                [(None, 30, 3)]      0           []                               \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 23, 64)       1600        ['A1[0][0]']                     \n",
            "                                                                                                  \n",
            " max_pooling1d (MaxPooling1D)   (None, 5, 64)        0           ['conv1d[0][0]']                 \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 320)          0           ['max_pooling1d[0][0]']          \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 50)           16050       ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " B1 (InputLayer)                [(None, 300)]        0           []                               \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 50)           0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " B2 (Dense)                     (None, 16)           4816        ['B1[0][0]']                     \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 66)           0           ['dropout[0][0]',                \n",
            "                                                                  'B2[0][0]']                     \n",
            "                                                                                                  \n",
            " M2 (Dense)                     (None, 1)            67          ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 22,533\n",
            "Trainable params: 22,533\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fit the neural network."
      ],
      "metadata": {
        "id": "T_StxCza4IRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=50, \n",
        "        verbose=1, mode='auto', restore_best_weights=True)\n",
        "\n",
        "cnn_history = model.fit(X_train, Y_train, callbacks=[monitor],\n",
        "    validation_data=(X_valid, Y_valid), epochs=epochs, verbose=2)"
      ],
      "metadata": {
        "id": "V5KiECgn1P4s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "147d9d30-833f-4c83-b9f4-d7e7de6ae646"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "260/260 - 14s - loss: 607.7997 - val_loss: 254.9702 - 14s/epoch - 52ms/step\n",
            "Epoch 2/500\n",
            "260/260 - 1s - loss: 263.4838 - val_loss: 178.3209 - 957ms/epoch - 4ms/step\n",
            "Epoch 3/500\n",
            "260/260 - 1s - loss: 212.5328 - val_loss: 160.2250 - 959ms/epoch - 4ms/step\n",
            "Epoch 4/500\n",
            "260/260 - 1s - loss: 198.9192 - val_loss: 149.7798 - 959ms/epoch - 4ms/step\n",
            "Epoch 5/500\n",
            "260/260 - 1s - loss: 194.6260 - val_loss: 185.7659 - 963ms/epoch - 4ms/step\n",
            "Epoch 6/500\n",
            "260/260 - 1s - loss: 181.9152 - val_loss: 151.6127 - 958ms/epoch - 4ms/step\n",
            "Epoch 7/500\n",
            "260/260 - 1s - loss: 179.9996 - val_loss: 164.6633 - 944ms/epoch - 4ms/step\n",
            "Epoch 8/500\n",
            "260/260 - 1s - loss: 179.5129 - val_loss: 137.1138 - 974ms/epoch - 4ms/step\n",
            "Epoch 9/500\n",
            "260/260 - 1s - loss: 169.4826 - val_loss: 142.2314 - 943ms/epoch - 4ms/step\n",
            "Epoch 10/500\n",
            "260/260 - 1s - loss: 171.4550 - val_loss: 152.0302 - 957ms/epoch - 4ms/step\n",
            "Epoch 11/500\n",
            "260/260 - 1s - loss: 170.5389 - val_loss: 137.8645 - 954ms/epoch - 4ms/step\n",
            "Epoch 12/500\n",
            "260/260 - 1s - loss: 159.6959 - val_loss: 133.2669 - 950ms/epoch - 4ms/step\n",
            "Epoch 13/500\n",
            "260/260 - 1s - loss: 165.9657 - val_loss: 143.0451 - 963ms/epoch - 4ms/step\n",
            "Epoch 14/500\n",
            "260/260 - 1s - loss: 168.1706 - val_loss: 133.9134 - 955ms/epoch - 4ms/step\n",
            "Epoch 15/500\n",
            "260/260 - 1s - loss: 159.1050 - val_loss: 137.8180 - 949ms/epoch - 4ms/step\n",
            "Epoch 16/500\n",
            "260/260 - 1s - loss: 172.2906 - val_loss: 135.6188 - 948ms/epoch - 4ms/step\n",
            "Epoch 17/500\n",
            "260/260 - 1s - loss: 156.9708 - val_loss: 139.1550 - 968ms/epoch - 4ms/step\n",
            "Epoch 18/500\n",
            "260/260 - 1s - loss: 164.9828 - val_loss: 138.5541 - 1s/epoch - 5ms/step\n",
            "Epoch 19/500\n",
            "260/260 - 1s - loss: 158.7398 - val_loss: 201.6272 - 926ms/epoch - 4ms/step\n",
            "Epoch 20/500\n",
            "260/260 - 1s - loss: 154.4290 - val_loss: 165.7910 - 944ms/epoch - 4ms/step\n",
            "Epoch 21/500\n",
            "260/260 - 1s - loss: 155.3896 - val_loss: 150.5770 - 940ms/epoch - 4ms/step\n",
            "Epoch 22/500\n",
            "260/260 - 1s - loss: 155.6733 - val_loss: 161.2397 - 961ms/epoch - 4ms/step\n",
            "Epoch 23/500\n",
            "260/260 - 1s - loss: 160.1925 - val_loss: 131.8377 - 932ms/epoch - 4ms/step\n",
            "Epoch 24/500\n",
            "260/260 - 1s - loss: 152.3947 - val_loss: 140.2563 - 961ms/epoch - 4ms/step\n",
            "Epoch 25/500\n",
            "260/260 - 1s - loss: 152.5278 - val_loss: 133.8041 - 945ms/epoch - 4ms/step\n",
            "Epoch 26/500\n",
            "260/260 - 1s - loss: 155.1156 - val_loss: 144.0747 - 949ms/epoch - 4ms/step\n",
            "Epoch 27/500\n",
            "260/260 - 1s - loss: 149.5668 - val_loss: 146.1867 - 942ms/epoch - 4ms/step\n",
            "Epoch 28/500\n",
            "260/260 - 1s - loss: 147.2475 - val_loss: 142.8648 - 930ms/epoch - 4ms/step\n",
            "Epoch 29/500\n",
            "260/260 - 1s - loss: 145.4615 - val_loss: 130.8866 - 954ms/epoch - 4ms/step\n",
            "Epoch 30/500\n",
            "260/260 - 1s - loss: 153.9766 - val_loss: 128.2615 - 951ms/epoch - 4ms/step\n",
            "Epoch 31/500\n",
            "260/260 - 1s - loss: 146.1938 - val_loss: 142.4355 - 971ms/epoch - 4ms/step\n",
            "Epoch 32/500\n",
            "260/260 - 1s - loss: 151.6871 - val_loss: 126.3834 - 954ms/epoch - 4ms/step\n",
            "Epoch 33/500\n",
            "260/260 - 1s - loss: 151.1238 - val_loss: 124.9919 - 941ms/epoch - 4ms/step\n",
            "Epoch 34/500\n",
            "260/260 - 1s - loss: 150.9275 - val_loss: 136.8627 - 976ms/epoch - 4ms/step\n",
            "Epoch 35/500\n",
            "260/260 - 1s - loss: 146.5035 - val_loss: 126.9269 - 964ms/epoch - 4ms/step\n",
            "Epoch 36/500\n",
            "260/260 - 1s - loss: 149.6058 - val_loss: 128.4392 - 962ms/epoch - 4ms/step\n",
            "Epoch 37/500\n",
            "260/260 - 1s - loss: 145.5752 - val_loss: 131.4435 - 953ms/epoch - 4ms/step\n",
            "Epoch 38/500\n",
            "260/260 - 1s - loss: 142.5260 - val_loss: 131.9208 - 946ms/epoch - 4ms/step\n",
            "Epoch 39/500\n",
            "260/260 - 1s - loss: 142.8273 - val_loss: 130.8022 - 945ms/epoch - 4ms/step\n",
            "Epoch 40/500\n",
            "260/260 - 1s - loss: 144.3962 - val_loss: 126.7103 - 969ms/epoch - 4ms/step\n",
            "Epoch 41/500\n",
            "260/260 - 1s - loss: 144.3366 - val_loss: 129.1287 - 941ms/epoch - 4ms/step\n",
            "Epoch 42/500\n",
            "260/260 - 1s - loss: 139.6154 - val_loss: 129.3092 - 940ms/epoch - 4ms/step\n",
            "Epoch 43/500\n",
            "260/260 - 1s - loss: 142.4147 - val_loss: 128.5397 - 972ms/epoch - 4ms/step\n",
            "Epoch 44/500\n",
            "260/260 - 1s - loss: 147.3140 - val_loss: 137.3976 - 955ms/epoch - 4ms/step\n",
            "Epoch 45/500\n",
            "260/260 - 1s - loss: 144.1534 - val_loss: 128.7100 - 974ms/epoch - 4ms/step\n",
            "Epoch 46/500\n",
            "260/260 - 1s - loss: 138.0874 - val_loss: 135.1030 - 928ms/epoch - 4ms/step\n",
            "Epoch 47/500\n",
            "260/260 - 1s - loss: 139.8817 - val_loss: 122.8664 - 941ms/epoch - 4ms/step\n",
            "Epoch 48/500\n",
            "260/260 - 1s - loss: 139.3114 - val_loss: 123.1378 - 979ms/epoch - 4ms/step\n",
            "Epoch 49/500\n",
            "260/260 - 1s - loss: 132.0528 - val_loss: 129.7079 - 956ms/epoch - 4ms/step\n",
            "Epoch 50/500\n",
            "260/260 - 1s - loss: 139.9159 - val_loss: 126.6239 - 956ms/epoch - 4ms/step\n",
            "Epoch 51/500\n",
            "260/260 - 1s - loss: 135.6781 - val_loss: 123.3426 - 926ms/epoch - 4ms/step\n",
            "Epoch 52/500\n",
            "260/260 - 1s - loss: 149.2729 - val_loss: 122.1562 - 944ms/epoch - 4ms/step\n",
            "Epoch 53/500\n",
            "260/260 - 1s - loss: 142.4686 - val_loss: 132.4063 - 926ms/epoch - 4ms/step\n",
            "Epoch 54/500\n",
            "260/260 - 1s - loss: 136.2394 - val_loss: 122.6592 - 952ms/epoch - 4ms/step\n",
            "Epoch 55/500\n",
            "260/260 - 1s - loss: 135.4149 - val_loss: 134.7387 - 938ms/epoch - 4ms/step\n",
            "Epoch 56/500\n",
            "260/260 - 1s - loss: 134.7010 - val_loss: 128.9042 - 1s/epoch - 5ms/step\n",
            "Epoch 57/500\n",
            "260/260 - 2s - loss: 137.3767 - val_loss: 148.4703 - 2s/epoch - 6ms/step\n",
            "Epoch 58/500\n",
            "260/260 - 1s - loss: 133.4819 - val_loss: 122.5145 - 975ms/epoch - 4ms/step\n",
            "Epoch 59/500\n",
            "260/260 - 1s - loss: 139.4129 - val_loss: 129.8188 - 955ms/epoch - 4ms/step\n",
            "Epoch 60/500\n",
            "260/260 - 1s - loss: 131.7760 - val_loss: 123.1314 - 948ms/epoch - 4ms/step\n",
            "Epoch 61/500\n",
            "260/260 - 1s - loss: 134.1069 - val_loss: 119.4796 - 940ms/epoch - 4ms/step\n",
            "Epoch 62/500\n",
            "260/260 - 1s - loss: 129.6641 - val_loss: 122.2801 - 964ms/epoch - 4ms/step\n",
            "Epoch 63/500\n",
            "260/260 - 1s - loss: 131.5226 - val_loss: 123.2157 - 930ms/epoch - 4ms/step\n",
            "Epoch 64/500\n",
            "260/260 - 1s - loss: 127.3711 - val_loss: 126.0366 - 938ms/epoch - 4ms/step\n",
            "Epoch 65/500\n",
            "260/260 - 1s - loss: 127.3526 - val_loss: 124.6295 - 953ms/epoch - 4ms/step\n",
            "Epoch 66/500\n",
            "260/260 - 1s - loss: 125.9618 - val_loss: 126.2916 - 939ms/epoch - 4ms/step\n",
            "Epoch 67/500\n",
            "260/260 - 1s - loss: 124.2375 - val_loss: 120.5786 - 955ms/epoch - 4ms/step\n",
            "Epoch 68/500\n",
            "260/260 - 1s - loss: 130.7116 - val_loss: 122.9263 - 949ms/epoch - 4ms/step\n",
            "Epoch 69/500\n",
            "260/260 - 1s - loss: 131.2598 - val_loss: 118.8475 - 935ms/epoch - 4ms/step\n",
            "Epoch 70/500\n",
            "260/260 - 1s - loss: 132.0480 - val_loss: 118.4542 - 954ms/epoch - 4ms/step\n",
            "Epoch 71/500\n",
            "260/260 - 1s - loss: 130.6698 - val_loss: 135.6477 - 929ms/epoch - 4ms/step\n",
            "Epoch 72/500\n",
            "260/260 - 1s - loss: 127.6900 - val_loss: 118.8618 - 927ms/epoch - 4ms/step\n",
            "Epoch 73/500\n",
            "260/260 - 1s - loss: 126.7853 - val_loss: 124.7249 - 942ms/epoch - 4ms/step\n",
            "Epoch 74/500\n",
            "260/260 - 1s - loss: 127.9087 - val_loss: 117.7558 - 975ms/epoch - 4ms/step\n",
            "Epoch 75/500\n",
            "260/260 - 1s - loss: 124.4395 - val_loss: 122.8767 - 939ms/epoch - 4ms/step\n",
            "Epoch 76/500\n",
            "260/260 - 1s - loss: 123.8374 - val_loss: 117.6959 - 943ms/epoch - 4ms/step\n",
            "Epoch 77/500\n",
            "260/260 - 1s - loss: 123.3228 - val_loss: 137.7831 - 926ms/epoch - 4ms/step\n",
            "Epoch 78/500\n",
            "260/260 - 1s - loss: 123.4305 - val_loss: 114.7967 - 952ms/epoch - 4ms/step\n",
            "Epoch 79/500\n",
            "260/260 - 1s - loss: 119.6418 - val_loss: 125.4495 - 943ms/epoch - 4ms/step\n",
            "Epoch 80/500\n",
            "260/260 - 1s - loss: 121.8127 - val_loss: 119.8554 - 958ms/epoch - 4ms/step\n",
            "Epoch 81/500\n",
            "260/260 - 1s - loss: 122.4317 - val_loss: 112.6203 - 966ms/epoch - 4ms/step\n",
            "Epoch 82/500\n",
            "260/260 - 1s - loss: 124.6121 - val_loss: 116.0784 - 943ms/epoch - 4ms/step\n",
            "Epoch 83/500\n",
            "260/260 - 1s - loss: 116.5976 - val_loss: 116.3956 - 951ms/epoch - 4ms/step\n",
            "Epoch 84/500\n",
            "260/260 - 1s - loss: 123.0592 - val_loss: 113.6674 - 956ms/epoch - 4ms/step\n",
            "Epoch 85/500\n",
            "260/260 - 1s - loss: 116.0302 - val_loss: 118.2299 - 968ms/epoch - 4ms/step\n",
            "Epoch 86/500\n",
            "260/260 - 1s - loss: 114.0049 - val_loss: 112.1429 - 956ms/epoch - 4ms/step\n",
            "Epoch 87/500\n",
            "260/260 - 1s - loss: 117.8636 - val_loss: 117.7725 - 960ms/epoch - 4ms/step\n",
            "Epoch 88/500\n",
            "260/260 - 1s - loss: 117.5005 - val_loss: 112.6124 - 938ms/epoch - 4ms/step\n",
            "Epoch 89/500\n",
            "260/260 - 1s - loss: 122.7862 - val_loss: 112.4418 - 936ms/epoch - 4ms/step\n",
            "Epoch 90/500\n",
            "260/260 - 1s - loss: 112.4453 - val_loss: 148.0292 - 957ms/epoch - 4ms/step\n",
            "Epoch 91/500\n",
            "260/260 - 1s - loss: 112.2457 - val_loss: 109.7821 - 981ms/epoch - 4ms/step\n",
            "Epoch 92/500\n",
            "260/260 - 1s - loss: 114.7279 - val_loss: 130.3913 - 948ms/epoch - 4ms/step\n",
            "Epoch 93/500\n",
            "260/260 - 1s - loss: 116.2408 - val_loss: 116.0310 - 958ms/epoch - 4ms/step\n",
            "Epoch 94/500\n",
            "260/260 - 1s - loss: 116.0407 - val_loss: 110.7551 - 942ms/epoch - 4ms/step\n",
            "Epoch 95/500\n",
            "260/260 - 1s - loss: 117.6117 - val_loss: 115.6074 - 934ms/epoch - 4ms/step\n",
            "Epoch 96/500\n",
            "260/260 - 1s - loss: 115.1752 - val_loss: 107.6725 - 965ms/epoch - 4ms/step\n",
            "Epoch 97/500\n",
            "260/260 - 1s - loss: 115.3365 - val_loss: 109.0240 - 952ms/epoch - 4ms/step\n",
            "Epoch 98/500\n",
            "260/260 - 1s - loss: 111.4445 - val_loss: 111.4218 - 953ms/epoch - 4ms/step\n",
            "Epoch 99/500\n",
            "260/260 - 1s - loss: 115.0533 - val_loss: 111.9457 - 972ms/epoch - 4ms/step\n",
            "Epoch 100/500\n",
            "260/260 - 1s - loss: 115.2243 - val_loss: 108.0787 - 940ms/epoch - 4ms/step\n",
            "Epoch 101/500\n",
            "260/260 - 1s - loss: 109.4974 - val_loss: 111.8776 - 934ms/epoch - 4ms/step\n",
            "Epoch 102/500\n",
            "260/260 - 1s - loss: 107.6417 - val_loss: 117.1647 - 968ms/epoch - 4ms/step\n",
            "Epoch 103/500\n",
            "260/260 - 1s - loss: 108.2976 - val_loss: 114.5866 - 942ms/epoch - 4ms/step\n",
            "Epoch 104/500\n",
            "260/260 - 1s - loss: 112.9865 - val_loss: 119.1464 - 950ms/epoch - 4ms/step\n",
            "Epoch 105/500\n",
            "260/260 - 1s - loss: 108.0132 - val_loss: 106.7820 - 928ms/epoch - 4ms/step\n",
            "Epoch 106/500\n",
            "260/260 - 1s - loss: 110.1771 - val_loss: 128.4199 - 986ms/epoch - 4ms/step\n",
            "Epoch 107/500\n",
            "260/260 - 1s - loss: 112.8769 - val_loss: 111.6055 - 950ms/epoch - 4ms/step\n",
            "Epoch 108/500\n",
            "260/260 - 1s - loss: 112.3930 - val_loss: 109.2921 - 948ms/epoch - 4ms/step\n",
            "Epoch 109/500\n",
            "260/260 - 1s - loss: 108.6927 - val_loss: 110.0849 - 955ms/epoch - 4ms/step\n",
            "Epoch 110/500\n",
            "260/260 - 1s - loss: 104.7863 - val_loss: 104.7354 - 971ms/epoch - 4ms/step\n",
            "Epoch 111/500\n",
            "260/260 - 1s - loss: 105.5550 - val_loss: 106.3677 - 954ms/epoch - 4ms/step\n",
            "Epoch 112/500\n",
            "260/260 - 1s - loss: 113.4372 - val_loss: 111.9962 - 941ms/epoch - 4ms/step\n",
            "Epoch 113/500\n",
            "260/260 - 1s - loss: 109.7412 - val_loss: 108.8127 - 968ms/epoch - 4ms/step\n",
            "Epoch 114/500\n",
            "260/260 - 1s - loss: 107.2978 - val_loss: 109.5508 - 943ms/epoch - 4ms/step\n",
            "Epoch 115/500\n",
            "260/260 - 1s - loss: 105.5533 - val_loss: 114.9572 - 943ms/epoch - 4ms/step\n",
            "Epoch 116/500\n",
            "260/260 - 1s - loss: 112.3499 - val_loss: 107.2900 - 948ms/epoch - 4ms/step\n",
            "Epoch 117/500\n",
            "260/260 - 1s - loss: 105.4884 - val_loss: 104.2183 - 941ms/epoch - 4ms/step\n",
            "Epoch 118/500\n",
            "260/260 - 1s - loss: 101.8511 - val_loss: 101.8912 - 935ms/epoch - 4ms/step\n",
            "Epoch 119/500\n",
            "260/260 - 1s - loss: 104.4872 - val_loss: 103.3747 - 932ms/epoch - 4ms/step\n",
            "Epoch 120/500\n",
            "260/260 - 1s - loss: 101.6070 - val_loss: 101.1521 - 969ms/epoch - 4ms/step\n",
            "Epoch 121/500\n",
            "260/260 - 1s - loss: 106.1397 - val_loss: 101.7509 - 921ms/epoch - 4ms/step\n",
            "Epoch 122/500\n",
            "260/260 - 1s - loss: 108.2493 - val_loss: 113.0657 - 933ms/epoch - 4ms/step\n",
            "Epoch 123/500\n",
            "260/260 - 1s - loss: 106.6683 - val_loss: 105.6620 - 949ms/epoch - 4ms/step\n",
            "Epoch 124/500\n",
            "260/260 - 1s - loss: 104.2062 - val_loss: 106.5849 - 920ms/epoch - 4ms/step\n",
            "Epoch 125/500\n",
            "260/260 - 1s - loss: 103.4866 - val_loss: 107.7272 - 916ms/epoch - 4ms/step\n",
            "Epoch 126/500\n",
            "260/260 - 1s - loss: 104.4321 - val_loss: 108.4902 - 931ms/epoch - 4ms/step\n",
            "Epoch 127/500\n",
            "260/260 - 1s - loss: 104.9385 - val_loss: 110.6901 - 944ms/epoch - 4ms/step\n",
            "Epoch 128/500\n",
            "260/260 - 1s - loss: 102.7625 - val_loss: 101.1477 - 954ms/epoch - 4ms/step\n",
            "Epoch 129/500\n",
            "260/260 - 1s - loss: 100.5196 - val_loss: 104.9471 - 967ms/epoch - 4ms/step\n",
            "Epoch 130/500\n",
            "260/260 - 1s - loss: 105.7253 - val_loss: 106.9665 - 926ms/epoch - 4ms/step\n",
            "Epoch 131/500\n",
            "260/260 - 1s - loss: 102.8080 - val_loss: 114.0476 - 978ms/epoch - 4ms/step\n",
            "Epoch 132/500\n",
            "260/260 - 1s - loss: 105.1308 - val_loss: 114.0879 - 950ms/epoch - 4ms/step\n",
            "Epoch 133/500\n",
            "260/260 - 1s - loss: 101.5429 - val_loss: 134.5170 - 949ms/epoch - 4ms/step\n",
            "Epoch 134/500\n",
            "260/260 - 1s - loss: 100.0881 - val_loss: 103.6411 - 948ms/epoch - 4ms/step\n",
            "Epoch 135/500\n",
            "260/260 - 1s - loss: 103.9394 - val_loss: 101.4393 - 948ms/epoch - 4ms/step\n",
            "Epoch 136/500\n",
            "260/260 - 1s - loss: 109.8068 - val_loss: 99.3993 - 951ms/epoch - 4ms/step\n",
            "Epoch 137/500\n",
            "260/260 - 1s - loss: 104.0136 - val_loss: 110.2044 - 965ms/epoch - 4ms/step\n",
            "Epoch 138/500\n",
            "260/260 - 1s - loss: 103.8686 - val_loss: 99.1826 - 983ms/epoch - 4ms/step\n",
            "Epoch 139/500\n",
            "260/260 - 1s - loss: 95.8642 - val_loss: 106.2616 - 946ms/epoch - 4ms/step\n",
            "Epoch 140/500\n",
            "260/260 - 1s - loss: 99.8028 - val_loss: 98.9042 - 963ms/epoch - 4ms/step\n",
            "Epoch 141/500\n",
            "260/260 - 1s - loss: 103.6154 - val_loss: 101.9725 - 926ms/epoch - 4ms/step\n",
            "Epoch 142/500\n",
            "260/260 - 1s - loss: 98.9770 - val_loss: 99.1787 - 942ms/epoch - 4ms/step\n",
            "Epoch 143/500\n",
            "260/260 - 1s - loss: 98.7368 - val_loss: 112.6435 - 954ms/epoch - 4ms/step\n",
            "Epoch 144/500\n",
            "260/260 - 1s - loss: 101.6082 - val_loss: 109.0409 - 943ms/epoch - 4ms/step\n",
            "Epoch 145/500\n",
            "260/260 - 1s - loss: 101.7303 - val_loss: 110.7960 - 955ms/epoch - 4ms/step\n",
            "Epoch 146/500\n",
            "260/260 - 1s - loss: 102.8883 - val_loss: 115.5948 - 949ms/epoch - 4ms/step\n",
            "Epoch 147/500\n",
            "260/260 - 1s - loss: 102.0330 - val_loss: 109.2430 - 940ms/epoch - 4ms/step\n",
            "Epoch 148/500\n",
            "260/260 - 1s - loss: 100.3297 - val_loss: 106.4307 - 971ms/epoch - 4ms/step\n",
            "Epoch 149/500\n",
            "260/260 - 1s - loss: 99.2768 - val_loss: 98.8158 - 955ms/epoch - 4ms/step\n",
            "Epoch 150/500\n",
            "260/260 - 1s - loss: 102.2899 - val_loss: 110.3491 - 917ms/epoch - 4ms/step\n",
            "Epoch 151/500\n",
            "260/260 - 1s - loss: 99.3466 - val_loss: 99.6100 - 947ms/epoch - 4ms/step\n",
            "Epoch 152/500\n",
            "260/260 - 1s - loss: 98.7697 - val_loss: 130.7571 - 948ms/epoch - 4ms/step\n",
            "Epoch 153/500\n",
            "260/260 - 1s - loss: 103.5564 - val_loss: 108.9616 - 959ms/epoch - 4ms/step\n",
            "Epoch 154/500\n",
            "260/260 - 1s - loss: 94.2802 - val_loss: 109.7668 - 970ms/epoch - 4ms/step\n",
            "Epoch 155/500\n",
            "260/260 - 1s - loss: 94.4504 - val_loss: 101.8065 - 955ms/epoch - 4ms/step\n",
            "Epoch 156/500\n",
            "260/260 - 1s - loss: 99.5272 - val_loss: 99.7445 - 939ms/epoch - 4ms/step\n",
            "Epoch 157/500\n",
            "260/260 - 1s - loss: 97.6052 - val_loss: 105.1757 - 952ms/epoch - 4ms/step\n",
            "Epoch 158/500\n",
            "260/260 - 1s - loss: 98.3330 - val_loss: 109.1037 - 964ms/epoch - 4ms/step\n",
            "Epoch 159/500\n",
            "260/260 - 1s - loss: 100.7595 - val_loss: 103.9207 - 965ms/epoch - 4ms/step\n",
            "Epoch 160/500\n",
            "260/260 - 1s - loss: 94.8786 - val_loss: 97.1068 - 957ms/epoch - 4ms/step\n",
            "Epoch 161/500\n",
            "260/260 - 1s - loss: 96.9363 - val_loss: 95.4844 - 939ms/epoch - 4ms/step\n",
            "Epoch 162/500\n",
            "260/260 - 1s - loss: 93.0054 - val_loss: 100.3094 - 962ms/epoch - 4ms/step\n",
            "Epoch 163/500\n",
            "260/260 - 1s - loss: 94.0703 - val_loss: 105.7679 - 956ms/epoch - 4ms/step\n",
            "Epoch 164/500\n",
            "260/260 - 1s - loss: 96.7411 - val_loss: 102.1302 - 940ms/epoch - 4ms/step\n",
            "Epoch 165/500\n",
            "260/260 - 1s - loss: 97.2683 - val_loss: 100.0718 - 950ms/epoch - 4ms/step\n",
            "Epoch 166/500\n",
            "260/260 - 1s - loss: 96.7842 - val_loss: 100.5222 - 950ms/epoch - 4ms/step\n",
            "Epoch 167/500\n",
            "260/260 - 1s - loss: 94.6844 - val_loss: 101.3411 - 967ms/epoch - 4ms/step\n",
            "Epoch 168/500\n",
            "260/260 - 1s - loss: 95.0968 - val_loss: 104.4633 - 947ms/epoch - 4ms/step\n",
            "Epoch 169/500\n",
            "260/260 - 1s - loss: 95.3749 - val_loss: 103.0289 - 1s/epoch - 5ms/step\n",
            "Epoch 170/500\n",
            "260/260 - 1s - loss: 99.1987 - val_loss: 95.0864 - 963ms/epoch - 4ms/step\n",
            "Epoch 171/500\n",
            "260/260 - 1s - loss: 98.1174 - val_loss: 98.0914 - 989ms/epoch - 4ms/step\n",
            "Epoch 172/500\n",
            "260/260 - 1s - loss: 95.0506 - val_loss: 98.8579 - 973ms/epoch - 4ms/step\n",
            "Epoch 173/500\n",
            "260/260 - 1s - loss: 94.1730 - val_loss: 95.1228 - 949ms/epoch - 4ms/step\n",
            "Epoch 174/500\n",
            "260/260 - 1s - loss: 90.7007 - val_loss: 97.6276 - 946ms/epoch - 4ms/step\n",
            "Epoch 175/500\n",
            "260/260 - 1s - loss: 92.9573 - val_loss: 96.6314 - 945ms/epoch - 4ms/step\n",
            "Epoch 176/500\n",
            "260/260 - 1s - loss: 96.2112 - val_loss: 104.3886 - 959ms/epoch - 4ms/step\n",
            "Epoch 177/500\n",
            "260/260 - 1s - loss: 96.4599 - val_loss: 96.8413 - 949ms/epoch - 4ms/step\n",
            "Epoch 178/500\n",
            "260/260 - 1s - loss: 97.4621 - val_loss: 104.1116 - 939ms/epoch - 4ms/step\n",
            "Epoch 179/500\n",
            "260/260 - 1s - loss: 100.7845 - val_loss: 134.6725 - 944ms/epoch - 4ms/step\n",
            "Epoch 180/500\n",
            "260/260 - 1s - loss: 95.8940 - val_loss: 100.8711 - 946ms/epoch - 4ms/step\n",
            "Epoch 181/500\n",
            "260/260 - 1s - loss: 101.3391 - val_loss: 101.0540 - 943ms/epoch - 4ms/step\n",
            "Epoch 182/500\n",
            "260/260 - 1s - loss: 94.1806 - val_loss: 100.5194 - 992ms/epoch - 4ms/step\n",
            "Epoch 183/500\n",
            "260/260 - 1s - loss: 93.9259 - val_loss: 104.9372 - 948ms/epoch - 4ms/step\n",
            "Epoch 184/500\n",
            "260/260 - 1s - loss: 97.1087 - val_loss: 101.6889 - 961ms/epoch - 4ms/step\n",
            "Epoch 185/500\n",
            "260/260 - 1s - loss: 99.9735 - val_loss: 96.8877 - 960ms/epoch - 4ms/step\n",
            "Epoch 186/500\n",
            "260/260 - 1s - loss: 95.9214 - val_loss: 93.9199 - 973ms/epoch - 4ms/step\n",
            "Epoch 187/500\n",
            "260/260 - 1s - loss: 95.2172 - val_loss: 111.2285 - 949ms/epoch - 4ms/step\n",
            "Epoch 188/500\n",
            "260/260 - 1s - loss: 92.2217 - val_loss: 100.3380 - 970ms/epoch - 4ms/step\n",
            "Epoch 189/500\n",
            "260/260 - 1s - loss: 95.1305 - val_loss: 100.0954 - 939ms/epoch - 4ms/step\n",
            "Epoch 190/500\n",
            "260/260 - 1s - loss: 94.7732 - val_loss: 103.2660 - 947ms/epoch - 4ms/step\n",
            "Epoch 191/500\n",
            "260/260 - 1s - loss: 95.1159 - val_loss: 100.1373 - 960ms/epoch - 4ms/step\n",
            "Epoch 192/500\n",
            "260/260 - 1s - loss: 91.5599 - val_loss: 106.0466 - 1s/epoch - 5ms/step\n",
            "Epoch 193/500\n",
            "260/260 - 1s - loss: 91.6075 - val_loss: 125.7309 - 979ms/epoch - 4ms/step\n",
            "Epoch 194/500\n",
            "260/260 - 1s - loss: 99.7267 - val_loss: 99.4059 - 931ms/epoch - 4ms/step\n",
            "Epoch 195/500\n",
            "260/260 - 1s - loss: 88.4840 - val_loss: 118.7965 - 977ms/epoch - 4ms/step\n",
            "Epoch 196/500\n",
            "260/260 - 1s - loss: 97.1502 - val_loss: 99.6160 - 942ms/epoch - 4ms/step\n",
            "Epoch 197/500\n",
            "260/260 - 1s - loss: 97.5019 - val_loss: 96.8036 - 946ms/epoch - 4ms/step\n",
            "Epoch 198/500\n",
            "260/260 - 1s - loss: 97.2632 - val_loss: 98.0210 - 946ms/epoch - 4ms/step\n",
            "Epoch 199/500\n",
            "260/260 - 1s - loss: 93.0297 - val_loss: 95.2753 - 947ms/epoch - 4ms/step\n",
            "Epoch 200/500\n",
            "260/260 - 1s - loss: 88.2886 - val_loss: 94.1533 - 1s/epoch - 5ms/step\n",
            "Epoch 201/500\n",
            "260/260 - 1s - loss: 96.3684 - val_loss: 100.0319 - 971ms/epoch - 4ms/step\n",
            "Epoch 202/500\n",
            "260/260 - 1s - loss: 95.8425 - val_loss: 97.7726 - 966ms/epoch - 4ms/step\n",
            "Epoch 203/500\n",
            "260/260 - 1s - loss: 94.8670 - val_loss: 105.8870 - 977ms/epoch - 4ms/step\n",
            "Epoch 204/500\n",
            "260/260 - 1s - loss: 87.7650 - val_loss: 100.4549 - 1s/epoch - 4ms/step\n",
            "Epoch 205/500\n",
            "260/260 - 1s - loss: 91.6501 - val_loss: 108.7411 - 975ms/epoch - 4ms/step\n",
            "Epoch 206/500\n",
            "260/260 - 1s - loss: 92.2356 - val_loss: 96.0620 - 950ms/epoch - 4ms/step\n",
            "Epoch 207/500\n",
            "260/260 - 1s - loss: 95.2065 - val_loss: 99.4130 - 956ms/epoch - 4ms/step\n",
            "Epoch 208/500\n",
            "260/260 - 1s - loss: 92.4636 - val_loss: 106.9164 - 967ms/epoch - 4ms/step\n",
            "Epoch 209/500\n",
            "260/260 - 1s - loss: 95.0346 - val_loss: 111.3370 - 921ms/epoch - 4ms/step\n",
            "Epoch 210/500\n",
            "260/260 - 1s - loss: 91.7279 - val_loss: 99.2140 - 922ms/epoch - 4ms/step\n",
            "Epoch 211/500\n",
            "260/260 - 1s - loss: 91.1480 - val_loss: 100.5115 - 946ms/epoch - 4ms/step\n",
            "Epoch 212/500\n",
            "260/260 - 1s - loss: 87.9221 - val_loss: 95.7697 - 947ms/epoch - 4ms/step\n",
            "Epoch 213/500\n",
            "260/260 - 1s - loss: 86.8440 - val_loss: 96.8140 - 1s/epoch - 5ms/step\n",
            "Epoch 214/500\n",
            "260/260 - 2s - loss: 93.0625 - val_loss: 115.0671 - 2s/epoch - 6ms/step\n",
            "Epoch 215/500\n",
            "260/260 - 1s - loss: 93.8425 - val_loss: 96.5292 - 1s/epoch - 4ms/step\n",
            "Epoch 216/500\n",
            "260/260 - 1s - loss: 84.3655 - val_loss: 105.4372 - 952ms/epoch - 4ms/step\n",
            "Epoch 217/500\n",
            "260/260 - 1s - loss: 92.7202 - val_loss: 92.5189 - 939ms/epoch - 4ms/step\n",
            "Epoch 218/500\n",
            "260/260 - 1s - loss: 85.6772 - val_loss: 101.2537 - 965ms/epoch - 4ms/step\n",
            "Epoch 219/500\n",
            "260/260 - 1s - loss: 89.4604 - val_loss: 94.4826 - 951ms/epoch - 4ms/step\n",
            "Epoch 220/500\n",
            "260/260 - 1s - loss: 90.8853 - val_loss: 105.1687 - 938ms/epoch - 4ms/step\n",
            "Epoch 221/500\n",
            "260/260 - 1s - loss: 89.8080 - val_loss: 107.4035 - 952ms/epoch - 4ms/step\n",
            "Epoch 222/500\n",
            "260/260 - 1s - loss: 89.6428 - val_loss: 98.9443 - 1s/epoch - 4ms/step\n",
            "Epoch 223/500\n",
            "260/260 - 1s - loss: 90.0980 - val_loss: 102.2277 - 968ms/epoch - 4ms/step\n",
            "Epoch 224/500\n",
            "260/260 - 1s - loss: 89.6808 - val_loss: 98.9416 - 948ms/epoch - 4ms/step\n",
            "Epoch 225/500\n",
            "260/260 - 1s - loss: 92.5335 - val_loss: 96.4707 - 930ms/epoch - 4ms/step\n",
            "Epoch 226/500\n",
            "260/260 - 1s - loss: 86.3903 - val_loss: 99.7509 - 943ms/epoch - 4ms/step\n",
            "Epoch 227/500\n",
            "260/260 - 1s - loss: 86.6613 - val_loss: 99.6645 - 946ms/epoch - 4ms/step\n",
            "Epoch 228/500\n",
            "260/260 - 1s - loss: 86.0513 - val_loss: 97.1780 - 947ms/epoch - 4ms/step\n",
            "Epoch 229/500\n",
            "260/260 - 1s - loss: 89.3211 - val_loss: 119.4752 - 955ms/epoch - 4ms/step\n",
            "Epoch 230/500\n",
            "260/260 - 1s - loss: 95.5821 - val_loss: 93.2933 - 974ms/epoch - 4ms/step\n",
            "Epoch 231/500\n",
            "260/260 - 1s - loss: 84.0881 - val_loss: 103.5280 - 968ms/epoch - 4ms/step\n",
            "Epoch 232/500\n",
            "260/260 - 1s - loss: 96.6335 - val_loss: 100.0671 - 961ms/epoch - 4ms/step\n",
            "Epoch 233/500\n",
            "260/260 - 1s - loss: 90.9237 - val_loss: 92.1937 - 941ms/epoch - 4ms/step\n",
            "Epoch 234/500\n",
            "260/260 - 1s - loss: 93.2547 - val_loss: 93.2849 - 960ms/epoch - 4ms/step\n",
            "Epoch 235/500\n",
            "260/260 - 1s - loss: 86.4706 - val_loss: 93.7172 - 944ms/epoch - 4ms/step\n",
            "Epoch 236/500\n",
            "260/260 - 1s - loss: 86.9562 - val_loss: 110.9041 - 932ms/epoch - 4ms/step\n",
            "Epoch 237/500\n",
            "260/260 - 1s - loss: 89.7091 - val_loss: 97.5032 - 925ms/epoch - 4ms/step\n",
            "Epoch 238/500\n",
            "260/260 - 1s - loss: 88.3345 - val_loss: 95.6465 - 951ms/epoch - 4ms/step\n",
            "Epoch 239/500\n",
            "260/260 - 1s - loss: 87.4971 - val_loss: 94.1730 - 939ms/epoch - 4ms/step\n",
            "Epoch 240/500\n",
            "260/260 - 1s - loss: 92.9043 - val_loss: 97.2435 - 933ms/epoch - 4ms/step\n",
            "Epoch 241/500\n",
            "260/260 - 1s - loss: 88.7857 - val_loss: 94.6833 - 934ms/epoch - 4ms/step\n",
            "Epoch 242/500\n",
            "260/260 - 1s - loss: 87.1679 - val_loss: 99.0973 - 943ms/epoch - 4ms/step\n",
            "Epoch 243/500\n",
            "260/260 - 1s - loss: 90.3201 - val_loss: 94.3216 - 947ms/epoch - 4ms/step\n",
            "Epoch 244/500\n",
            "260/260 - 1s - loss: 89.1864 - val_loss: 97.1427 - 945ms/epoch - 4ms/step\n",
            "Epoch 245/500\n",
            "260/260 - 1s - loss: 82.0718 - val_loss: 97.0777 - 964ms/epoch - 4ms/step\n",
            "Epoch 246/500\n",
            "260/260 - 1s - loss: 87.3858 - val_loss: 98.4169 - 957ms/epoch - 4ms/step\n",
            "Epoch 247/500\n",
            "260/260 - 1s - loss: 89.5664 - val_loss: 91.7656 - 930ms/epoch - 4ms/step\n",
            "Epoch 248/500\n",
            "260/260 - 1s - loss: 88.1811 - val_loss: 91.9861 - 959ms/epoch - 4ms/step\n",
            "Epoch 249/500\n",
            "260/260 - 1s - loss: 90.6167 - val_loss: 93.3351 - 937ms/epoch - 4ms/step\n",
            "Epoch 250/500\n",
            "260/260 - 1s - loss: 86.7358 - val_loss: 102.3888 - 932ms/epoch - 4ms/step\n",
            "Epoch 251/500\n",
            "260/260 - 1s - loss: 89.4195 - val_loss: 101.4434 - 959ms/epoch - 4ms/step\n",
            "Epoch 252/500\n",
            "260/260 - 1s - loss: 86.5655 - val_loss: 96.4614 - 962ms/epoch - 4ms/step\n",
            "Epoch 253/500\n",
            "260/260 - 1s - loss: 87.5847 - val_loss: 98.5174 - 961ms/epoch - 4ms/step\n",
            "Epoch 254/500\n",
            "260/260 - 1s - loss: 88.2350 - val_loss: 96.7864 - 953ms/epoch - 4ms/step\n",
            "Epoch 255/500\n",
            "260/260 - 1s - loss: 86.2130 - val_loss: 108.6589 - 931ms/epoch - 4ms/step\n",
            "Epoch 256/500\n",
            "260/260 - 1s - loss: 89.5357 - val_loss: 90.6474 - 972ms/epoch - 4ms/step\n",
            "Epoch 257/500\n",
            "260/260 - 1s - loss: 85.4221 - val_loss: 90.7682 - 953ms/epoch - 4ms/step\n",
            "Epoch 258/500\n",
            "260/260 - 1s - loss: 93.9264 - val_loss: 95.8614 - 921ms/epoch - 4ms/step\n",
            "Epoch 259/500\n",
            "260/260 - 1s - loss: 83.3673 - val_loss: 96.0235 - 944ms/epoch - 4ms/step\n",
            "Epoch 260/500\n",
            "260/260 - 1s - loss: 91.4949 - val_loss: 95.6817 - 938ms/epoch - 4ms/step\n",
            "Epoch 261/500\n",
            "260/260 - 1s - loss: 90.5593 - val_loss: 104.0860 - 958ms/epoch - 4ms/step\n",
            "Epoch 262/500\n",
            "260/260 - 1s - loss: 83.3020 - val_loss: 91.7501 - 970ms/epoch - 4ms/step\n",
            "Epoch 263/500\n",
            "260/260 - 1s - loss: 84.4133 - val_loss: 94.1361 - 956ms/epoch - 4ms/step\n",
            "Epoch 264/500\n",
            "260/260 - 1s - loss: 83.3298 - val_loss: 104.7329 - 948ms/epoch - 4ms/step\n",
            "Epoch 265/500\n",
            "260/260 - 1s - loss: 87.8186 - val_loss: 90.0587 - 964ms/epoch - 4ms/step\n",
            "Epoch 266/500\n",
            "260/260 - 1s - loss: 85.0377 - val_loss: 92.3407 - 980ms/epoch - 4ms/step\n",
            "Epoch 267/500\n",
            "260/260 - 1s - loss: 83.5969 - val_loss: 104.4959 - 937ms/epoch - 4ms/step\n",
            "Epoch 268/500\n",
            "260/260 - 1s - loss: 85.9912 - val_loss: 103.5753 - 949ms/epoch - 4ms/step\n",
            "Epoch 269/500\n",
            "260/260 - 1s - loss: 84.8285 - val_loss: 103.9802 - 954ms/epoch - 4ms/step\n",
            "Epoch 270/500\n",
            "260/260 - 1s - loss: 84.7836 - val_loss: 93.1621 - 966ms/epoch - 4ms/step\n",
            "Epoch 271/500\n",
            "260/260 - 1s - loss: 90.0342 - val_loss: 95.7798 - 926ms/epoch - 4ms/step\n",
            "Epoch 272/500\n",
            "260/260 - 1s - loss: 86.5756 - val_loss: 100.5612 - 941ms/epoch - 4ms/step\n",
            "Epoch 273/500\n",
            "260/260 - 1s - loss: 88.3198 - val_loss: 95.2624 - 959ms/epoch - 4ms/step\n",
            "Epoch 274/500\n",
            "260/260 - 1s - loss: 85.8894 - val_loss: 95.8987 - 942ms/epoch - 4ms/step\n",
            "Epoch 275/500\n",
            "260/260 - 1s - loss: 86.2929 - val_loss: 115.2142 - 964ms/epoch - 4ms/step\n",
            "Epoch 276/500\n",
            "260/260 - 1s - loss: 87.3904 - val_loss: 90.9092 - 922ms/epoch - 4ms/step\n",
            "Epoch 277/500\n",
            "260/260 - 1s - loss: 83.2456 - val_loss: 95.3939 - 959ms/epoch - 4ms/step\n",
            "Epoch 278/500\n",
            "260/260 - 1s - loss: 86.1840 - val_loss: 92.6188 - 921ms/epoch - 4ms/step\n",
            "Epoch 279/500\n",
            "260/260 - 1s - loss: 83.0269 - val_loss: 97.9701 - 942ms/epoch - 4ms/step\n",
            "Epoch 280/500\n",
            "260/260 - 1s - loss: 84.0409 - val_loss: 91.4868 - 931ms/epoch - 4ms/step\n",
            "Epoch 281/500\n",
            "260/260 - 1s - loss: 82.4304 - val_loss: 93.0106 - 947ms/epoch - 4ms/step\n",
            "Epoch 282/500\n",
            "260/260 - 1s - loss: 88.6104 - val_loss: 101.6736 - 932ms/epoch - 4ms/step\n",
            "Epoch 283/500\n",
            "260/260 - 1s - loss: 87.3347 - val_loss: 93.6162 - 947ms/epoch - 4ms/step\n",
            "Epoch 284/500\n",
            "260/260 - 1s - loss: 84.5952 - val_loss: 97.9202 - 954ms/epoch - 4ms/step\n",
            "Epoch 285/500\n",
            "260/260 - 1s - loss: 85.3250 - val_loss: 93.0532 - 928ms/epoch - 4ms/step\n",
            "Epoch 286/500\n",
            "260/260 - 1s - loss: 82.9367 - val_loss: 91.5220 - 943ms/epoch - 4ms/step\n",
            "Epoch 287/500\n",
            "260/260 - 1s - loss: 86.7226 - val_loss: 109.9155 - 957ms/epoch - 4ms/step\n",
            "Epoch 288/500\n",
            "260/260 - 1s - loss: 85.0616 - val_loss: 97.9702 - 942ms/epoch - 4ms/step\n",
            "Epoch 289/500\n",
            "260/260 - 1s - loss: 81.8990 - val_loss: 94.4727 - 939ms/epoch - 4ms/step\n",
            "Epoch 290/500\n",
            "260/260 - 1s - loss: 82.5117 - val_loss: 97.0129 - 939ms/epoch - 4ms/step\n",
            "Epoch 291/500\n",
            "260/260 - 1s - loss: 85.7981 - val_loss: 97.0410 - 955ms/epoch - 4ms/step\n",
            "Epoch 292/500\n",
            "260/260 - 1s - loss: 83.2320 - val_loss: 102.1485 - 922ms/epoch - 4ms/step\n",
            "Epoch 293/500\n",
            "260/260 - 1s - loss: 83.6039 - val_loss: 94.3200 - 955ms/epoch - 4ms/step\n",
            "Epoch 294/500\n",
            "260/260 - 1s - loss: 85.1321 - val_loss: 97.0267 - 955ms/epoch - 4ms/step\n",
            "Epoch 295/500\n",
            "260/260 - 1s - loss: 84.8339 - val_loss: 93.1170 - 941ms/epoch - 4ms/step\n",
            "Epoch 296/500\n",
            "260/260 - 1s - loss: 85.0821 - val_loss: 104.4238 - 943ms/epoch - 4ms/step\n",
            "Epoch 297/500\n",
            "260/260 - 1s - loss: 83.9049 - val_loss: 96.0777 - 938ms/epoch - 4ms/step\n",
            "Epoch 298/500\n",
            "260/260 - 1s - loss: 85.4043 - val_loss: 97.4130 - 942ms/epoch - 4ms/step\n",
            "Epoch 299/500\n",
            "260/260 - 1s - loss: 83.9211 - val_loss: 107.7723 - 922ms/epoch - 4ms/step\n",
            "Epoch 300/500\n",
            "260/260 - 1s - loss: 86.9503 - val_loss: 91.8295 - 918ms/epoch - 4ms/step\n",
            "Epoch 301/500\n",
            "260/260 - 1s - loss: 85.7597 - val_loss: 96.4879 - 944ms/epoch - 4ms/step\n",
            "Epoch 302/500\n",
            "260/260 - 1s - loss: 81.3415 - val_loss: 96.5175 - 958ms/epoch - 4ms/step\n",
            "Epoch 303/500\n",
            "260/260 - 1s - loss: 81.0619 - val_loss: 96.3574 - 925ms/epoch - 4ms/step\n",
            "Epoch 304/500\n",
            "260/260 - 1s - loss: 80.9569 - val_loss: 98.2869 - 938ms/epoch - 4ms/step\n",
            "Epoch 305/500\n",
            "260/260 - 1s - loss: 83.8227 - val_loss: 94.0676 - 943ms/epoch - 4ms/step\n",
            "Epoch 306/500\n",
            "260/260 - 1s - loss: 83.8936 - val_loss: 95.0762 - 940ms/epoch - 4ms/step\n",
            "Epoch 307/500\n",
            "260/260 - 1s - loss: 87.7247 - val_loss: 94.4359 - 935ms/epoch - 4ms/step\n",
            "Epoch 308/500\n",
            "260/260 - 1s - loss: 81.2939 - val_loss: 91.5515 - 951ms/epoch - 4ms/step\n",
            "Epoch 309/500\n",
            "260/260 - 1s - loss: 87.2786 - val_loss: 94.8718 - 955ms/epoch - 4ms/step\n",
            "Epoch 310/500\n",
            "260/260 - 1s - loss: 82.0057 - val_loss: 97.7697 - 953ms/epoch - 4ms/step\n",
            "Epoch 311/500\n",
            "260/260 - 1s - loss: 80.8350 - val_loss: 94.8849 - 922ms/epoch - 4ms/step\n",
            "Epoch 312/500\n",
            "260/260 - 1s - loss: 83.0299 - val_loss: 96.6438 - 956ms/epoch - 4ms/step\n",
            "Epoch 313/500\n",
            "260/260 - 1s - loss: 81.0173 - val_loss: 99.2183 - 951ms/epoch - 4ms/step\n",
            "Epoch 314/500\n",
            "260/260 - 1s - loss: 82.0512 - val_loss: 99.9436 - 950ms/epoch - 4ms/step\n",
            "Epoch 315/500\n",
            "Restoring model weights from the end of the best epoch: 265.\n",
            "260/260 - 1s - loss: 86.9313 - val_loss: 99.7841 - 944ms/epoch - 4ms/step\n",
            "Epoch 315: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predict and evaluate the validation data."
      ],
      "metadata": {
        "id": "O-Q08cT94MU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "cnn_train_pred = model.predict(X_train)\n",
        "cnn_valid_pred = model.predict(X_valid)\n",
        "print('Train rmse:', np.sqrt(mean_squared_error(Y_train, cnn_train_pred)))\n",
        "print('Validation rmse:', np.sqrt(mean_squared_error(Y_valid, cnn_valid_pred)))"
      ],
      "metadata": {
        "id": "gyUB1nSG9fhr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6b0b8cc-c0e6-4e6c-dce0-ecd5a8e537d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train rmse: 7.379589355065541\n",
            "Validation rmse: 9.489926125574819\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train rmse: 8.260069977016887\n",
        "Validation rmse: 10.943058830677673\n",
        "\n",
        "Plot the training curve."
      ],
      "metadata": {
        "id": "wYNHiuuk4QAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#fig, axes = plt.subplots(2, 2, sharex=True, sharey=True,figsize=(22,12))\n",
        "#ax1, ax2 = axes[0]\n",
        "#ax3, ax4 = axes[1]\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(cnn_history.history['loss'], label='Train loss')\n",
        "plt.plot(cnn_history.history['val_loss'], label='Validation loss')\n",
        "fig.legend()\n",
        "fig.suptitle('CNN')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"MSE\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qbAvkOwmCdao",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "e8d821ae-67e2-4eaa-f15c-8faa1bed5637"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}