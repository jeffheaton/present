{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeffheaton/present/blob/master/WUSTL/CABI-Demand/demand_nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Washington University [Olin School of Business](https://olin.wustl.edu/EN-US/Pages/default.aspx)\n",
    "[Center for Analytics and Business Insights](https://olin.wustl.edu/EN-US/Faculty-Research/research-centers/center-analytics-business-insights/Pages/default.aspx) (CABI)  \n",
    "[Deep Learning for Demand Forecasting](https://github.com/jeffheaton/present/tree/master/WUSTL/CABI-Demand)  \n",
    "Copyright 2022 by [Jeff Heaton](https://www.youtube.com/c/HeatonResearch), Released under [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/) \n",
    "\n",
    "## NLP Projection\n",
    "\n",
    "First map Google drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EJPIlxl8hAg5",
    "outputId": "0706a391-2207-4274-a5d4-a75e3d2994ed"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cvkEW0rIZjS"
   },
   "source": [
    "### Load the Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IdE224AVJHxa",
    "outputId": "6a5acb5f-9e45-40f7-9cac-8047c64b1295"
   },
   "outputs": [],
   "source": [
    "!wget -c \"https://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "!unzip /content/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bhBfLkU-Nt3L"
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove_file = '/content/glove.6B.300d.txt'\n",
    "tmp_file = get_tmpfile(\"test_word2vec.txt\")\n",
    "_ = glove2word2vec(glove_file, tmp_file)\n",
    "w2vec_model = KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uwgl1rkuQaRn"
   },
   "source": [
    "### NLP Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "immf5pMjQdRC",
    "outputId": "0f788c1f-239c-454e-a03b-7f9cf834b6a2"
   },
   "outputs": [],
   "source": [
    "w2vec_model.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SJPkTG-WRCw9",
    "outputId": "eeef2545-7181-4824-903f-87bc7c06f51b"
   },
   "outputs": [],
   "source": [
    "w2vec_model['dog'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yN7476ZB3MTi"
   },
   "source": [
    "### Perform NLP Prediction\n",
    "Load the three data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5WEfyt5ShSeA"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "PATH = \"/content/drive/MyDrive/projects/demand/\"\n",
    "\n",
    "df_sales = pd.read_csv(\"https://data.heatonresearch.com/wustl/CABI/demand-forecast/sales_train.csv\", parse_dates=['date'])\n",
    "df_items = pd.read_csv(\"https://data.heatonresearch.com/wustl/CABI/demand-forecast/items.csv\")\n",
    "df_resturant = pd.read_csv(\"https://data.heatonresearch.com/wustl/CABI/demand-forecast/resturants.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bS1m-S85RPp2",
    "outputId": "f45f695a-4418-4592-abc3-b0ca8e3f30c2"
   },
   "outputs": [],
   "source": [
    "def process_title(model, name):\n",
    "  v = None\n",
    "  i = 0\n",
    "  for word in name.split(' '):\n",
    "    if word == 'vegi': word = \"vegetable\"\n",
    "    if word == 'smoothy': word = \"malt\"\n",
    "    i+=1\n",
    "    if v is None:\n",
    "      v=model[word].copy()\n",
    "    else:\n",
    "      v+=model[word]\n",
    "  v/=i\n",
    "  return v\n",
    "\n",
    "item_lookup = {}\n",
    "for i, name in zip(list(df_items.id),list(df_items.name)):\n",
    "  v = process_title(w2vec_model,name)\n",
    "  item_lookup[i] = v\n",
    "\n",
    "#r = process_title(model, 'breaded fish with vegetables meal')\n",
    "print(len(item_lookup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__jGovBB3QUR"
   },
   "source": [
    "Utility function to create sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kz2MmH3Cloi7"
   },
   "outputs": [],
   "source": [
    "def series_to_supervised(data, window=1, lag=1, dropnan=True):\n",
    "    cols, names = list(), list()\n",
    "    # Input sequence (t-n, ... t-1)\n",
    "    for i in range(window, 0, -1):\n",
    "        cols.append(data.shift(i))\n",
    "        names += [('%s(t-%d)' % (col, i)) for col in data.columns]\n",
    "    # Current timestep (t=0)\n",
    "    cols.append(data)\n",
    "    names += [('%s(t)' % (col)) for col in data.columns]\n",
    "    # Target timestep (t=lag)\n",
    "    cols.append(data.shift(-lag))\n",
    "    names += [('%s(t+%d)' % (col, lag)) for col in data.columns]\n",
    "    # Put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # Drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DmL154wv3YIR"
   },
   "source": [
    "Join the items and sales tables so that we can look up the store id for each item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "3S7LIIqG3ihU",
    "outputId": "a1524568-9b20-478d-e3ee-584908139c44"
   },
   "outputs": [],
   "source": [
    "df_items2 = df_items[['id','store_id']]\n",
    "df_train = df_sales.merge(df_items2,left_on='item_id',right_on='id')\n",
    "df_train[['date','item_id','item_count','store_id']]\n",
    "\n",
    "df_train = df_train.sort_values('date').groupby(['item_id', 'store_id', 'date'], as_index=False)\n",
    "df_train = df_train.agg({'item_count':['mean']})\n",
    "df_train.columns = ['item', 'store', 'date', 'sales']\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "juImdYgR-rNi",
    "outputId": "b4952149-2c0b-4170-bcc1-bb6f549cc073"
   },
   "outputs": [],
   "source": [
    "df_train['dow'] = df_train['date'].dt.dayofweek\n",
    "df_train['doy'] = df_train['date'].dt.dayofyear\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSslf7_x3f_r"
   },
   "source": [
    "Build the sequence data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "ZKf1jppTlvi4",
    "outputId": "b1368ba9-ee8c-4e3c-8880-dbeb5fbc2bbb"
   },
   "outputs": [],
   "source": [
    "window = 29\n",
    "future_span = 30\n",
    "series = series_to_supervised(df_train.drop('date', axis=1), window=window, lag=future_span)\n",
    "series.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gMBAkJvR3nce"
   },
   "source": [
    "Remove sequences that did not have enough data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_6D8BDtu-DWy"
   },
   "outputs": [],
   "source": [
    "# Remove edge cases, where there were not enough values to complete a series\n",
    "last_item = 'item(t-%d)' % window\n",
    "last_store = 'store(t-%d)' % window\n",
    "last_dow = 'dow(t-%d)' % window\n",
    "last_doy = 'doy(t-%d)' % window\n",
    "series = series[(series['store(t)'] == series[last_store])]\n",
    "series = series[(series['item(t)'] == series[last_item])]\n",
    "#series = series[(series['dow(t)'] == series[last_dow])]\n",
    "#series = series[(series['doy(t)'] == series[last_doy])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4quQLgwg3xfM"
   },
   "source": [
    "We will predict with sales, and our engineered features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RRhDirsoZ4dZ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.utils.np_utils import to_categorical   \n",
    "\n",
    "def drop_column(df, col):\n",
    "  columns_to_drop = [('%s(t+%d)' % (col, future_span))]\n",
    "  for i in range(window, 0, -1):\n",
    "      columns_to_drop += [('%s(t-%d)' % (col, i))]\n",
    "  df.drop(columns_to_drop, axis=1, inplace=True, errors='ignore')\n",
    "  df.drop([f\"{col}(t)\"], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "def cat_seq(df, col):\n",
    "  return to_categorical(df[col].values)\n",
    "\n",
    "# Label\n",
    "labels_col = 'sales(t+%d)' % future_span\n",
    "labels = series[labels_col]\n",
    "series.drop(labels_col, axis=1, inplace=True)\n",
    "series.drop('item(t+%d)' % future_span, axis=1, inplace=True)\n",
    "series.drop('store(t+%d)' % future_span, axis=1, inplace=True)\n",
    "series.drop('dow(t+%d)' % future_span, axis=1, inplace=True)\n",
    "series.drop('doy(t+%d)' % future_span, axis=1, inplace=True)\n",
    "\n",
    "# Get sales sequences\n",
    "series2 = series.copy()\n",
    "drop_column(series2, \"item\")\n",
    "drop_column(series2, \"store\")\n",
    "drop_column(series2, \"dow\")\n",
    "drop_column(series2, \"doy\")\n",
    "sales_series = series2.values\n",
    "\n",
    "# Day of week as a number\n",
    "series2 = series.copy()\n",
    "drop_column(series2, \"item\")\n",
    "drop_column(series2, \"store\")\n",
    "drop_column(series2, \"doy\")\n",
    "drop_column(series2, \"sales\")\n",
    "dow_series = series2.values\n",
    "\n",
    "# Get day of year sequences\n",
    "series2 = series.copy()\n",
    "drop_column(series2, \"item\")\n",
    "drop_column(series2, \"store\")\n",
    "drop_column(series2, \"dow\")\n",
    "drop_column(series2, \"sales\")\n",
    "doy_series = series2.values\n",
    "\n",
    "# Day of year\n",
    "t1 = sales_series.reshape(sales_series.shape + (1,))\n",
    "t2 = dow_series.reshape(dow_series.shape + (1,)) \n",
    "t3 = doy_series.reshape(doy_series.shape + (1,))\n",
    "\n",
    "# Create predictors (x)\n",
    "vec_size = w2vec_model['test'].shape[0]\n",
    "\n",
    "lst = []\n",
    "for item in list(series['item(t-1)']):\n",
    "  lst.append(item_lookup[item])\n",
    "\n",
    "x1 = np.concatenate([t1,t2,t3],axis=2)\n",
    "x2 = np.concatenate(lst).reshape((series.shape[0],vec_size))\n",
    "\n",
    "x = [x1,x2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J0gn1hXDk4jY",
    "outputId": "4826b971-f047-4e90-eef4-fea7ed331270"
   },
   "outputs": [],
   "source": [
    "print(t1.shape)\n",
    "print(t2.shape)\n",
    "print(t3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3mrMep9homxN"
   },
   "outputs": [],
   "source": [
    "#store_series = to_categorical(series['store(t)'].values)\n",
    "#store_series.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLrgz7RK33f1"
   },
   "source": [
    "Extract the predictors (x sequences) and the label (future prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ntMujMSFj4v_",
    "outputId": "0464c2ce-08f4-4079-97b7-fcd9c3963032"
   },
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.4\n",
    "\n",
    "mask = np.random.random(size=x[0].shape[0])<TEST_SIZE\n",
    "\n",
    "X_train = []\n",
    "X_valid = []\n",
    "\n",
    "for subx in x:\n",
    "  X_train.append(subx[~mask])\n",
    "  X_valid.append(subx[mask])\n",
    "\n",
    "Y_train = labels.values[~mask]\n",
    "Y_valid = labels.values[mask]\n",
    "\n",
    "print('Train set shape x1:', X_train[0].shape)\n",
    "print('Train set shape x2:', X_train[1].shape)\n",
    "print('Validation set shape x1:', X_valid[0].shape)\n",
    "print('Validation set shape x2:', X_valid[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IazoyOgL4D5T"
   },
   "source": [
    "Construct the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iSX-MNvjz81I",
    "outputId": "42e073c7-b153-4681-bc2c-24f70953f625"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers import Dense, LSTM, RepeatVector, TimeDistributed, Flatten, Dropout, concatenate, Input\n",
    "import keras\n",
    "\n",
    "epochs = 500\n",
    "batch = 256\n",
    "lr = 0.0003\n",
    "adam = tf.keras.optimizers.Adam(lr)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "A1 = Input(shape=(X_train[0].shape[1], X_train[0].shape[2]),name='A1')\n",
    "A2 = Conv1D(filters=64, kernel_size=8, activation='relu')(A1)\n",
    "A3 = MaxPooling1D(pool_size=4)(A2)\n",
    "A4 = Flatten()(A3)\n",
    "A5 = Dense(50, activation='relu')(A4)\n",
    "A6 = Dropout(0.2)(A5)\n",
    "\n",
    "B1 = Input(shape=X_train[1].shape[1],name='B1')\n",
    "B2 = Dense(16, activation='relu',name='B2')(B1)\n",
    "\n",
    "M1 = concatenate([A6,B2])\n",
    "M2 = Dense(1,name='M2')(M1)\n",
    "\n",
    "model = Model(inputs=[A1, B1],outputs=[M2])\n",
    "model.compile(loss='mse', optimizer=adam)\n",
    "model.summary()\n",
    "\n",
    "#model = Sequential()\n",
    "#model.add(LSTM(50, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "#model.add(Dense(1))\n",
    "#model.compile(loss='mse', optimizer=adam)\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_StxCza4IRx"
   },
   "source": [
    "Fit the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V5KiECgn1P4s",
    "outputId": "147d9d30-833f-4c83-b9f4-d7e7de6ae646"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=50, \n",
    "        verbose=1, mode='auto', restore_best_weights=True)\n",
    "\n",
    "cnn_history = model.fit(X_train, Y_train, callbacks=[monitor],\n",
    "    validation_data=(X_valid, Y_valid), epochs=epochs, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-Q08cT94MU5"
   },
   "source": [
    "Predict and evaluate the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gyUB1nSG9fhr",
    "outputId": "d6b0b8cc-c0e6-4e6c-dce0-ecd5a8e537d6"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "cnn_train_pred = model.predict(X_train)\n",
    "cnn_valid_pred = model.predict(X_valid)\n",
    "print('Train rmse:', np.sqrt(mean_squared_error(Y_train, cnn_train_pred)))\n",
    "print('Validation rmse:', np.sqrt(mean_squared_error(Y_valid, cnn_valid_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wYNHiuuk4QAk"
   },
   "source": [
    "Train rmse: 8.260069977016887\n",
    "Validation rmse: 10.943058830677673\n",
    "\n",
    "Plot the training curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "id": "qbAvkOwmCdao",
    "outputId": "e8d821ae-67e2-4eaa-f15c-8faa1bed5637"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#fig, axes = plt.subplots(2, 2, sharex=True, sharey=True,figsize=(22,12))\n",
    "#ax1, ax2 = axes[0]\n",
    "#ax3, ax4 = axes[1]\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(cnn_history.history['loss'], label='Train loss')\n",
    "plt.plot(cnn_history.history['val_loss'], label='Validation loss')\n",
    "fig.legend()\n",
    "fig.suptitle('CNN')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"MSE\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
