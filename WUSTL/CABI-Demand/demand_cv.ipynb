{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeffheaton/present/blob/master/WUSTL/CABI-Demand/demand_cv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Washington University [Olin School of Business](https://olin.wustl.edu/EN-US/Pages/default.aspx)\n",
    "[Center for Analytics and Business Insights](https://olin.wustl.edu/EN-US/Faculty-Research/research-centers/center-analytics-business-insights/Pages/default.aspx) (CABI)  \n",
    "[Deep Learning for Demand Forecasting](https://github.com/jeffheaton/present/tree/master/WUSTL/CABI-Demand)  \n",
    "Copyright 2022 by [Jeff Heaton](https://www.youtube.com/c/HeatonResearch), Released under [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/)  \n",
    "\n",
    "\n",
    "## Demand Projection with Computer Vision\n",
    "\n",
    "Also includes the NLP from previous module. \n",
    "\n",
    "First map Google drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EJPIlxl8hAg5",
    "outputId": "07e9c58c-c35c-405f-92d1-beeb44875c71"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yN7476ZB3MTi"
   },
   "source": [
    "Load the three data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5WEfyt5ShSeA"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "PATH = \"/content/drive/MyDrive/projects/demand/\"\n",
    "\n",
    "df_sales = pd.read_csv(\"https://data.heatonresearch.com/wustl/CABI/demand-forecast/sales_train.csv\", parse_dates=['date'])\n",
    "df_items = pd.read_csv(\"https://data.heatonresearch.com/wustl/CABI/demand-forecast/items.csv\")\n",
    "df_resturant = pd.read_csv(\"https://data.heatonresearch.com/wustl/CABI/demand-forecast/resturants.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15pndRemtFvT"
   },
   "source": [
    "### Download Street View Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6JeXn4qktNX_",
    "outputId": "be844b58-d5a6-4aec-d90b-795aee4d6a48"
   },
   "outputs": [],
   "source": [
    "!wget https://data.heatonresearch.com/wustl/CABI/demand-forecast/street-20221003.zip /content/\n",
    "!unzip /content/street-20221003.zip > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WenPgHwls4hL"
   },
   "source": [
    "### Install YOLOv5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YCFo8Jn7AO2v",
    "outputId": "f15d39fa-bd4d-4a48-be38-b3bc1354c925"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "!git clone https://github.com/ultralytics/yolov5 --tag 6.2  # clone\n",
    "!mv /content/6.2 /content/yolov5\n",
    "%pip install -qr /content/yolov5/requirements.txt  # install\n",
    "sys.path.insert(0,'/content/yolov5')\n",
    "\n",
    "import torch\n",
    "import utils\n",
    "display = utils.notebook_init()  # checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ceCx1t7bu800"
   },
   "source": [
    "### Demonstrate YOLOv5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E4NgFr3txdTJ",
    "outputId": "4fc6530e-d6c8-40e4-a571-283521b8aee6"
   },
   "outputs": [],
   "source": [
    "!rm -R /content/yolov5/runs/detect/*\n",
    "!mkdir /content/images\n",
    "!cp /content/street/2020_01_01.jpg /content/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "id": "uyALr12NvB_P",
    "outputId": "09872567-e4ec-4dec-fed2-d9d90315ee6d"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "!python /content/yolov5/detect.py --weights yolov5s.pt --img 1024 \\\n",
    "  --conf 0.25 --source /content/images/\n",
    "\n",
    "URL = '/content/yolov5/runs/detect/exp/2020_01_01.jpg'\n",
    "Image(filename=URL, width=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 773
    },
    "id": "R1rB0mQyw2pW",
    "outputId": "ce7e26f6-0fb6-4509-9a24-f3a199d3e6c6"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Model\n",
    "yolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # or yolov5n - yolov5x6, custom\n",
    "\n",
    "# Images\n",
    "img = '/content/street/2020_01_01.jpg'  # or file, Path, PIL, OpenCV, numpy, list\n",
    "\n",
    "# Inference\n",
    "results = yolo_model(img)\n",
    "\n",
    "# Results\n",
    "df = results.pandas().xyxy[0]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcr13zWDuZFO"
   },
   "source": [
    "### Extract Data from Street Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "oLsgzfKDucxS",
    "outputId": "4a5c0dc8-440f-46dd-9929-95b141bf47a2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os import walk\n",
    "import datetime\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "STREET_PATH = \"/content/street/\"\n",
    "\n",
    "filenames = next(walk(STREET_PATH), (None, None, []))[2]  \n",
    "\n",
    "list_date = []\n",
    "list_people = []\n",
    "\n",
    "for file in tqdm.tqdm(filenames):\n",
    "  filename = os.path.join(STREET_PATH, file)\n",
    "  results = yolo_model(filename)\n",
    "  df = results.pandas().xyxy[0]\n",
    "  people = len(df[df.name=='person'])\n",
    "  dt = datetime.datetime.strptime(file[:10], '%Y_%m_%d')\n",
    "  list_date.append(dt)\n",
    "  list_people.append(people)\n",
    "\n",
    "df_street_view = pd.DataFrame({'date':list_date,'people':list_people})\n",
    "df_street_view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_bsHJwF_8DU"
   },
   "source": [
    "# Neural Network Code from Last Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eOAXxrOd2i0o"
   },
   "outputs": [],
   "source": [
    "def process_title(model, name):\n",
    "  v = None\n",
    "  i = 0\n",
    "  for word in name.split(' '):\n",
    "    if word == 'vegi': word = \"vegetable\"\n",
    "    if word == 'smoothy': word = \"malt\"\n",
    "    i+=1\n",
    "    if v is None:\n",
    "      v=model[word].copy()\n",
    "    else:\n",
    "      v+=model[word]\n",
    "  v/=i\n",
    "  return v\n",
    "\n",
    "def series_to_supervised(data, window=1, lag=1, dropnan=True):\n",
    "    cols, names = list(), list()\n",
    "    # Input sequence (t-n, ... t-1)\n",
    "    for i in range(window, 0, -1):\n",
    "        cols.append(data.shift(i))\n",
    "        names += [('%s(t-%d)' % (col, i)) for col in data.columns]\n",
    "    # Current timestep (t=0)\n",
    "    cols.append(data)\n",
    "    names += [('%s(t)' % (col)) for col in data.columns]\n",
    "    # Target timestep (t=lag)\n",
    "    cols.append(data.shift(-lag))\n",
    "    names += [('%s(t+%d)' % (col, lag)) for col in data.columns]\n",
    "    # Put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # Drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "    \n",
    "def drop_column(df, col):\n",
    "  columns_to_drop = [('%s(t+%d)' % (col, future_span))]\n",
    "  for i in range(window, 0, -1):\n",
    "      columns_to_drop += [('%s(t-%d)' % (col, i))]\n",
    "  df.drop(columns_to_drop, axis=1, inplace=True, errors='ignore')\n",
    "  df.drop([f\"{col}(t)\"], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "def cat_seq(df, col):\n",
    "  return to_categorical(df[col].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cvkEW0rIZjS"
   },
   "source": [
    "### Load the Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IdE224AVJHxa",
    "outputId": "dca4544a-96d9-49fa-87f4-14a7b9b0a848"
   },
   "outputs": [],
   "source": [
    "!wget -c \"https://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "!unzip /content/glove.6B.zip\n",
    "\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove_file = '/content/glove.6B.300d.txt'\n",
    "tmp_file = get_tmpfile(\"test_word2vec.txt\")\n",
    "_ = glove2word2vec(glove_file, tmp_file)\n",
    "w2vec_model = KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "bS1m-S85RPp2",
    "outputId": "0f01b44d-92e0-4eb2-e2fb-921a5f3ab977"
   },
   "outputs": [],
   "source": [
    "item_lookup = {}\n",
    "for i, name in zip(list(df_items.id),list(df_items.name)):\n",
    "  v = process_title(w2vec_model,name)\n",
    "  item_lookup[i] = v\n",
    "\n",
    "# Join the items and sales tables so that we can look up the store id for each item.\n",
    "df_items2 = df_items[['id','store_id']]\n",
    "df_train = df_sales.merge(df_items2,left_on='item_id',right_on='id')\n",
    "df_train[['date','item_id','item_count','store_id']]\n",
    "\n",
    "# Merge people counts (new)\n",
    "temp = len(df_train)\n",
    "df_train = df_train.merge(df_street_view)\n",
    "assert len(df_train) == temp\n",
    "\n",
    "# Sort/agg\n",
    "df_train = df_train.sort_values('date').groupby(['item_id', 'store_id', 'date'], as_index=False)\n",
    "df_train = df_train.agg({'item_count':['mean'],'people':['mean']})\n",
    "df_train.columns = ['item', 'store', 'date', 'sales','people']\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__jGovBB3QUR"
   },
   "source": [
    "# Engineer Time Series Features (new):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "3S7LIIqG3ihU",
    "outputId": "14eeb699-fa75-4d12-be30-25f6e5f2e563"
   },
   "outputs": [],
   "source": [
    "df_train['dow'] = df_train['date'].dt.dayofweek\n",
    "df_train['doy'] = df_train['date'].dt.dayofyear\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSslf7_x3f_r"
   },
   "source": [
    "# Build the sequence data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZKf1jppTlvi4"
   },
   "outputs": [],
   "source": [
    "window = 29\n",
    "future_span = 30\n",
    "series = series_to_supervised(df_train.drop('date', axis=1), window=window, lag=future_span)\n",
    "\n",
    "# Remove edge cases, where there were not enough values to complete a series\n",
    "last_item = 'item(t-%d)' % window\n",
    "last_store = 'store(t-%d)' % window\n",
    "last_dow = 'dow(t-%d)' % window\n",
    "last_doy = 'doy(t-%d)' % window\n",
    "series = series[(series['store(t)'] == series[last_store])]\n",
    "series = series[(series['item(t)'] == series[last_item])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4quQLgwg3xfM"
   },
   "source": [
    "We will predict with sales, and our engineered features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RRhDirsoZ4dZ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.utils.np_utils import to_categorical   \n",
    "\n",
    "# Label\n",
    "labels_col = 'sales(t+%d)' % future_span\n",
    "labels = series[labels_col]\n",
    "series.drop(labels_col, axis=1, inplace=True)\n",
    "series.drop('item(t+%d)' % future_span, axis=1, inplace=True)\n",
    "series.drop('store(t+%d)' % future_span, axis=1, inplace=True)\n",
    "series.drop('dow(t+%d)' % future_span, axis=1, inplace=True)\n",
    "series.drop('doy(t+%d)' % future_span, axis=1, inplace=True)\n",
    "series.drop('people(t+%d)' % future_span, axis=1, inplace=True)\n",
    "\n",
    "# Get sales sequences\n",
    "series2 = series.copy()\n",
    "drop_column(series2, \"item\")\n",
    "drop_column(series2, \"store\")\n",
    "drop_column(series2, \"dow\")\n",
    "drop_column(series2, \"doy\")\n",
    "drop_column(series2, \"people\")\n",
    "sales_series = series2.values\n",
    "\n",
    "# Day of week as a number\n",
    "series2 = series.copy()\n",
    "drop_column(series2, \"item\")\n",
    "drop_column(series2, \"store\")\n",
    "drop_column(series2, \"doy\")\n",
    "drop_column(series2, \"sales\")\n",
    "drop_column(series2, \"people\")\n",
    "dow_series = series2.values\n",
    "\n",
    "# Get day of year sequences\n",
    "series2 = series.copy()\n",
    "drop_column(series2, \"item\")\n",
    "drop_column(series2, \"store\")\n",
    "drop_column(series2, \"dow\")\n",
    "drop_column(series2, \"sales\")\n",
    "drop_column(series2, \"people\")\n",
    "doy_series = series2.values\n",
    "\n",
    "# Get number of people sequences\n",
    "series2 = series.copy()\n",
    "drop_column(series2, \"item\")\n",
    "drop_column(series2, \"store\")\n",
    "drop_column(series2, \"dow\")\n",
    "drop_column(series2, \"sales\")\n",
    "drop_column(series2, \"doy\")\n",
    "people_series = series2.values\n",
    "\n",
    "\n",
    "# Create x\n",
    "t1 = sales_series.reshape(sales_series.shape + (1,))\n",
    "t2 = dow_series.reshape(dow_series.shape + (1,)) \n",
    "t3 = doy_series.reshape(doy_series.shape + (1,))\n",
    "t4 = people_series.reshape(people_series.shape + (1,))\n",
    "x1 = np.concatenate([t1,t2,t3,t4],axis=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x-AW6pFrQ_0N",
    "outputId": "412e0ab6-c157-4b8f-dcd6-c5ee0e1436d0"
   },
   "outputs": [],
   "source": [
    "print(t1.shape)\n",
    "print(t2.shape)\n",
    "print(t3.shape)\n",
    "print(t4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J0gn1hXDk4jY"
   },
   "outputs": [],
   "source": [
    "# Create predictors (x)\n",
    "vec_size = w2vec_model['test'].shape[0]\n",
    "\n",
    "lst = []\n",
    "for item in list(series['item(t-1)']):\n",
    "  lst.append(item_lookup[item])\n",
    "\n",
    "x2 = np.concatenate(lst).reshape((series.shape[0],vec_size))\n",
    "\n",
    "x = [x1,x2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLrgz7RK33f1"
   },
   "source": [
    "# Train the Network\n",
    "Extract the predictors (x sequences) and the label (future prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ntMujMSFj4v_",
    "outputId": "30f0daa1-5572-4b8e-a6db-51069b30ef95"
   },
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.4\n",
    "\n",
    "mask = np.random.random(size=x[0].shape[0])<TEST_SIZE\n",
    "\n",
    "X_train = []\n",
    "X_valid = []\n",
    "\n",
    "for subx in x:\n",
    "  X_train.append(subx[~mask])\n",
    "  X_valid.append(subx[mask])\n",
    "\n",
    "Y_train = labels.values[~mask]\n",
    "Y_valid = labels.values[mask]\n",
    "\n",
    "print('Train set shape x1:', X_train[0].shape)\n",
    "print('Train set shape x2:', X_train[1].shape)\n",
    "print('Validation set shape x1:', X_valid[0].shape)\n",
    "print('Validation set shape x2:', X_valid[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IazoyOgL4D5T"
   },
   "source": [
    "Construct the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iSX-MNvjz81I",
    "outputId": "02561bef-8ce5-42fe-d6a4-40743d1cac96"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers import Dense, LSTM, RepeatVector, TimeDistributed, Flatten, Dropout, concatenate, Input\n",
    "import keras\n",
    "\n",
    "epochs = 500\n",
    "batch = 256\n",
    "lr = 0.0003\n",
    "adam = tf.keras.optimizers.Adam(lr)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "A1 = Input(shape=(X_train[0].shape[1], X_train[0].shape[2]),name='A1')\n",
    "A2 = Conv1D(filters=64, kernel_size=8, activation='relu')(A1)\n",
    "A3 = MaxPooling1D(pool_size=2)(A2)\n",
    "A4 = Flatten()(A3)\n",
    "A5 = Dense(50, activation='relu')(A4)\n",
    "A6 = Dropout(0.2)(A5)\n",
    "\n",
    "B1 = Input(shape=X_train[1].shape[1],name='B1')\n",
    "B2 = Dense(16, activation='relu',name='B2')(B1)\n",
    "\n",
    "M1 = concatenate([A6,B2])\n",
    "M2 = Dense(1,name='M2')(M1)\n",
    "\n",
    "model = Model(inputs=[A1, B1],outputs=[M2])\n",
    "model.compile(loss='mse', optimizer=adam)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_StxCza4IRx"
   },
   "source": [
    "Fit the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V5KiECgn1P4s",
    "outputId": "5d86f3a5-4d16-4940-e5af-91aed516af45"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=50, \n",
    "        verbose=1, mode='auto', restore_best_weights=True)\n",
    "\n",
    "cnn_history = model.fit(X_train, Y_train, callbacks=[monitor],\n",
    "    validation_data=(X_valid, Y_valid), epochs=epochs, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-Q08cT94MU5"
   },
   "source": [
    "Predict and evaluate the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gyUB1nSG9fhr",
    "outputId": "c6049bb4-e7f9-42cd-9a6e-4011889c99fe"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "cnn_train_pred = model.predict(X_train)\n",
    "cnn_valid_pred = model.predict(X_valid)\n",
    "print('Train rmse:', np.sqrt(mean_squared_error(Y_train, cnn_train_pred)))\n",
    "print('Validation rmse:', np.sqrt(mean_squared_error(Y_valid, cnn_valid_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wYNHiuuk4QAk"
   },
   "source": [
    "Train rmse: 8.260069977016887\n",
    "Validation rmse: 10.943058830677673\n",
    "\n",
    "Plot the training curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qbAvkOwmCdao"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(cnn_history.history['loss'], label='Train loss')\n",
    "plt.plot(cnn_history.history['val_loss'], label='Validation loss')\n",
    "fig.legend()\n",
    "fig.suptitle('CNN')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"MSE\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
