{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9V_9RChmQGs7"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeffheaton/present/blob/master/youtube/automl/simple-automl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "\n",
    "# Simple AutoML\n",
    "\n",
    "Copyright 2023 by [Jeff Heaton](https://youtube.com/@HeatonResearch), LGPL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "htVks4vk_BPL"
   },
   "outputs": [],
   "source": [
    "#DATA_SOURCE = \"https://data.heatonresearch.com/data/t81-558/auto-mpg.csv\"; TARGET = \"mpg\";IS_REGRESSION=True\n",
    "#DATA_SOURCE = \"https://data.heatonresearch.com/data/t81-558/iris.csv\"; TARGET = \"species\";IS_REGRESSION=False\n",
    "DATA_SOURCE = \"https://data.heatonresearch.com/data/t81-558/crx.csv\"; TARGET = 'a16'; IS_REGRESSION=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z21pOD98_kQc"
   },
   "source": [
    "# Analyze Data\n",
    "\n",
    "The following code implements the ```analyze``` function that determines the types and encodings of all columns in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "DSCFz5EBU4DN"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import shapiro\n",
    "import pandas as pd\n",
    "\n",
    "CONFIG_MAX_DUMMY = \"max_dummy\"\n",
    "CONFIG_MAX_DUMMY_PCT = \"pct_dummy\"\n",
    "\n",
    "CONFIG = {\n",
    "  CONFIG_MAX_DUMMY: 1000,\n",
    "  CONFIG_MAX_DUMMY_PCT: 0.75\n",
    "}\n",
    "\n",
    "def isnumeric(datatype):\n",
    "  return datatype in [FIELD_TYPE_FLOAT,FIELD_TYPE_INT]\n",
    "\n",
    "FIELDS = \"fields\"\n",
    "FIELD_ACTION = \"action\"\n",
    "FIELD_ACTION_COPY = \"copy\"\n",
    "FIELD_ACTION_IGNORE = \"ignore\"\n",
    "FIELD_ACTION_ZSCORE = \"zscore\"\n",
    "FIELD_ACTION_NORMALIZE = \"normalize\"\n",
    "FIELD_ACTION_DUMMY = \"dummy\"\n",
    "FIELD_ACTION_TARGET = \"target\"\n",
    "FIELD_NAME = \"name\"\n",
    "FIELD_SUM = \"sum\"\n",
    "FIELD_TYPE = \"type\"\n",
    "FIELD_MEAN = \"mean\"\n",
    "FIELD_NUM = \"n\"\n",
    "FIELD_MISSING = \"missing\"\n",
    "FIELD_MIN = \"min\"\n",
    "FIELD_MAX = \"max\"\n",
    "FIELD_VAR = \"var\"\n",
    "FIELD_SD = \"sd\"\n",
    "FIELD_UNIQUE = \"unique\"\n",
    "FIELD_MEDIAN = \"median\"\n",
    "FIELD_MODE = \"mode\"\n",
    "FIELD_SHAPIRO_STAT = \"shapiro-stat\"\n",
    "FIELD_SHAPIRO_P = \"shapiro-p\"\n",
    "META_TARGET = \"target\"\n",
    "META_TYPE = \"type\"\n",
    "META_TYPE_BINARY_CLASSIFICATION = \"binary-classification\"\n",
    "META_TYPE_CLASSIFICATION = \"classification\"\n",
    "META_TYPE_REGRESSION = \"regression\"\n",
    "META_SOURCE = \"source\"\n",
    "META_POSITIVE_TOKEN = \"positive-token\"\n",
    "META_EARLY_STOP = \"early-stop\"\n",
    "\n",
    "FIELD_TYPE_FLOAT = \"float\"\n",
    "FIELD_TYPE_INT = \"int\"\n",
    "FIELD_TYPE_STR = \"str\"\n",
    "\n",
    "def find_positive(s):\n",
    "  s = set(s.str.upper().tolist())\n",
    "  if len(s) != 2: return None\n",
    "  if \"+\" in s and \"-\" in s: return \"+\"\n",
    "  if \"0\" in s and \"1\" in s: return \"1\"\n",
    "  if \"t\" in s and \"f\" in s: return \"t\"\n",
    "  if \"y\" in s and \"n\" in s: return \"y\"\n",
    "  if \"true\" in s and \"false\" in s: return \"true\"\n",
    "  if \"yes\" in s and \"no\" in s: return \"yes\"\n",
    "  if \"p\" in s and \"n\" in s: return \"p\"\n",
    "  if \"positive\" in s and \"negative\" in s: return \"positive\"\n",
    "  s = list(s)\n",
    "  s.sort()\n",
    "  return s[0]\n",
    "\n",
    "def analyze(data_source, target, is_regression=True):\n",
    "  df = pd.read_csv(data_source,na_values=['NA', '?'])\n",
    "\n",
    "  metadata = {\n",
    "      FIELDS: {},\n",
    "      META_TARGET: target,\n",
    "      META_SOURCE: data_source,\n",
    "      META_EARLY_STOP: True\n",
    "  }\n",
    "\n",
    "  fields = metadata[FIELDS]\n",
    "\n",
    "  for field_name,csv_type in zip(df.columns,df.dtypes):\n",
    "    #print(name,csv_type)\n",
    "    if \"float\" in csv_type.name:\n",
    "      dtype = FIELD_TYPE_FLOAT\n",
    "      action = FIELD_ACTION_COPY\n",
    "    elif \"int\" in csv_type.name:\n",
    "      dtype = FIELD_TYPE_INT\n",
    "      action = FIELD_ACTION_COPY\n",
    "    else:\n",
    "      dtype = FIELD_TYPE_STR\n",
    "      action = FIELD_ACTION_IGNORE\n",
    "\n",
    "    missing_count = sum(df[field_name].isnull())\n",
    "    col = df[field_name]\n",
    "    unique_count = len(pd.unique(col))\n",
    "\n",
    "    if isnumeric(dtype):\n",
    "      stat, p = shapiro(col)\n",
    "\n",
    "      # less than or equal to 0.05 not normal\n",
    "      action = FIELD_ACTION_ZSCORE if p>0.05 else FIELD_ACTION_NORMALIZE\n",
    "\n",
    "      fields[field_name] = {\n",
    "          FIELD_TYPE:dtype,\n",
    "          FIELD_MEDIAN:col.median(),\n",
    "          FIELD_MEAN:col.mean(),\n",
    "          FIELD_SD:col.std(),\n",
    "          FIELD_MAX:col.max(),\n",
    "          FIELD_MIN:col.min(),\n",
    "          FIELD_SHAPIRO_STAT:stat,\n",
    "          FIELD_SHAPIRO_P:p,\n",
    "          FIELD_ACTION:action,\n",
    "          FIELD_MISSING:missing_count,\n",
    "          FIELD_UNIQUE:unique_count}\n",
    "\n",
    "    else:\n",
    "      fields[field_name] = {\n",
    "          FIELD_TYPE:dtype,\n",
    "          FIELD_MODE:col.mode()[0],\n",
    "          FIELD_ACTION:action,\n",
    "          FIELD_MISSING:missing_count,\n",
    "          FIELD_UNIQUE:unique_count}\n",
    "\n",
    "    # Determine action\n",
    "    field = fields[field_name]\n",
    "    if (field[FIELD_TYPE] == FIELD_TYPE_STR) and (field[FIELD_UNIQUE]<CONFIG[CONFIG_MAX_DUMMY]) and (field[FIELD_UNIQUE]/len(df)<CONFIG[CONFIG_MAX_DUMMY_PCT]):\n",
    "      field[FIELD_ACTION] = FIELD_ACTION_DUMMY\n",
    "    if field_name == target:\n",
    "      field[FIELD_ACTION] = FIELD_ACTION_TARGET\n",
    "  \n",
    "  # Determine model type\n",
    "  is_binary = (metadata[FIELDS][target][FIELD_UNIQUE]==2) and not is_regression\n",
    "\n",
    "  if is_regression:\n",
    "    metadata[META_TYPE] = META_TYPE_REGRESSION\n",
    "  else:\n",
    "    if metadata[FIELDS][target][FIELD_UNIQUE]==2:\n",
    "      metadata[META_TYPE] = META_TYPE_BINARY_CLASSIFICATION\n",
    "\n",
    "      metadata[META_POSITIVE_TOKEN] = find_positive(df[target])\n",
    "    else:\n",
    "      metadata[META_TYPE] = META_TYPE_CLASSIFICATION\n",
    "\n",
    "  return metadata\n",
    "\n",
    "COLS = [FIELD_MEAN, FIELD_SD, FIELD_MEDIAN, FIELD_MODE, FIELD_MAX, FIELD_ACTION, FIELD_UNIQUE, FIELD_SHAPIRO_P,FIELD_MISSING]\n",
    "\n",
    "def field_summary(metadata, cols=COLS):\n",
    "  data = {}\n",
    "\n",
    "  data['name'] = []\n",
    "  for col in cols:\n",
    "    data[col] = []\n",
    "\n",
    "  for field_name in metadata[FIELDS]:\n",
    "    field = metadata[FIELDS][field_name]\n",
    "    data['name'].append(field_name)\n",
    "    for col in cols:\n",
    "      data[col].append(field.get(col, None))\n",
    "\n",
    "  return pd.DataFrame(data)[['name']+COLS]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZOJ_jcYE234"
   },
   "source": [
    "# Generate Code\n",
    "\n",
    "The following code generates Keras Python code for the data that was analzyed in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "U3p2kabMhX1G"
   },
   "outputs": [],
   "source": [
    "from dataclasses import MISSING\n",
    "from pandas.core.dtypes.inference import is_re\n",
    "def tolist(obj):\n",
    "  if isinstance(obj,list) or isinstance(obj, tuple):\n",
    "    return obj\n",
    "  else:\n",
    "    return [obj]\n",
    "\n",
    "class PythonFile:\n",
    "  def __init__(self):\n",
    "    self.imports = []\n",
    "    self.lines = []\n",
    "\n",
    "  def add_import(self, name, alias=None):\n",
    "    if alias:\n",
    "      self.imports.append({\"name\": name, \"alias\": alias})\n",
    "    else:\n",
    "      self.imports.append({\"name\": name})\n",
    "\n",
    "  def add_from(self, _from, _import):\n",
    "    self.imports.append({\"from\": _from, \"import\": _import})\n",
    "    \n",
    "  def generate(self):\n",
    "    src = \"\"\n",
    "    for obj in self.imports:\n",
    "      if \"name\" in obj and \"alias\" in obj:\n",
    "        src += f\"import {obj['name']} as {obj['alias']}\"\n",
    "      elif \"name\" in obj and \"alias\" not in obj:\n",
    "        src += f\"import {obj['name']}\"\n",
    "      elif \"from\" in obj and \"import\" in obj:\n",
    "        imports = \", \".join(tolist(obj['import']))\n",
    "        src += f\"from {obj['from']} import {imports}\"\n",
    "\n",
    "      src+=\"\\n\"\n",
    "\n",
    "    for line in self.lines:\n",
    "      src+=line+\"\\n\"\n",
    "    return src\n",
    "\n",
    "  def add_line(self, str):\n",
    "    self.lines.append(str)\n",
    "\n",
    "  def comment(self, str):\n",
    "    return f\"# {str}\"\n",
    "\n",
    "  def call(self, name, *args):\n",
    "    src = name + \"(\"\n",
    "\n",
    "    formatted_args = []\n",
    "    started_named = False\n",
    "    for arg in args:\n",
    "      if isinstance(arg,dict):\n",
    "        formatted_args += [f\"{name}={arg[name]}\" for name in arg.keys()]\n",
    "        started_named = True\n",
    "      else: \n",
    "        if started_named: raise ValueError(\"positional argument follows keyword argument\")\n",
    "        formatted_args.append(str(arg))\n",
    "\n",
    "    src += \", \".join(formatted_args)\n",
    "    src += \")\"\n",
    "    return src\n",
    "\n",
    "  def assign(self, left, right):\n",
    "    return f\"{left} = {right}\"\n",
    "\n",
    "  def str(self, str):\n",
    "    return f\"\\\"{str}\\\"\"\n",
    "\n",
    "  def index(self, name, indexes, dot=None):\n",
    "    src = name\n",
    "    for idx in indexes:\n",
    "      src+=f'[{idx}]'\n",
    "\n",
    "    if dot:\n",
    "      src+='.'\n",
    "      src+=dot\n",
    "    return src\n",
    "    \n",
    "def generate_keras(metadata):\n",
    "  na_values = ['NA', '?']\n",
    "  target = metadata[META_TARGET]\n",
    "  is_regression = metadata[META_TYPE] == META_TYPE_REGRESSION\n",
    "  is_binary = (metadata[FIELDS][target][FIELD_UNIQUE]==2) and (metadata[META_TYPE]==META_TYPE_CLASSIFICATION)\n",
    "\n",
    "  if metadata[META_TYPE] == META_TYPE_REGRESSION:\n",
    "    loss = \"mean_squared_error\"\n",
    "  elif metadata[META_TYPE] == META_TYPE_BINARY_CLASSIFICATION:\n",
    "    loss = \"binary_crossentropy\"\n",
    "  else:\n",
    "    loss = \"categorical_crossentropy\"\n",
    "\n",
    "  py = PythonFile()\n",
    "  # Imports\n",
    "  py.add_import(\"pandas\", \"pd\")\n",
    "  py.add_import(\"io\")\n",
    "  py.add_import(\"requests\")\n",
    "  py.add_import(\"numpy\", \"np\")\n",
    "  py.add_from(\"tensorflow.keras.models\", \"Sequential\")\n",
    "  py.add_from(\"tensorflow.keras.layers\", [\"Dense\", \"Activation\"])\n",
    "  py.add_from(\"tensorflow.keras.callbacks\", \"EarlyStopping\")\n",
    "  py.add_from(\"scipy.stats\", \"zscore\")\n",
    "  py.add_from(\"sklearn.preprocessing\", \"MinMaxScaler\")\n",
    "\n",
    "  py.add_line(py.assign(\"df\", py.call(\"pd.read_csv\",py.str(metadata[META_SOURCE]),{'na_values':na_values})))\n",
    "  x_fields = [x for x in metadata[FIELDS] if x != target and metadata[FIELDS][x][FIELD_ACTION] in [FIELD_ACTION_COPY]]\n",
    "  py.add_line(py.assign(\"x_fields\",x_fields))\n",
    "  \n",
    "  # Analyze input columns\n",
    "  for field_name in metadata[FIELDS]:\n",
    "    field = metadata[FIELDS][field_name]\n",
    "    if field[FIELD_MISSING]>0:\n",
    "      if isnumeric(field[FIELD_TYPE]):\n",
    "        fn = \"median\"\n",
    "        suffix = \"\"\n",
    "      else:\n",
    "        fn = \"mode\"\n",
    "        suffix = \"[0]\"\n",
    "      py.add_line(py.assign(py.index(\"df\",[py.str(field_name)]),\n",
    "                py.index(\"df\",[py.str(field_name)],py.call(\"fillna\",\n",
    "                py.index(\"df\",[py.str(field_name)],py.call(fn)+suffix)\n",
    "                ))))\n",
    "    if field[FIELD_ACTION] == FIELD_ACTION_ZSCORE:\n",
    "      py.add_line(py.assign(py.index(\"df\",[py.str(field_name)]),\n",
    "                py.call(\"zscore\",py.index(\"df\",[py.str(field_name)]))))\n",
    "      py.add_line(py.call(\"x_fields.append\",py.str(field_name)))\n",
    "    elif field[FIELD_ACTION] == FIELD_ACTION_NORMALIZE:\n",
    "      f1 = py.index(\"df\",[py.str(field_name)])\n",
    "      f2 = py.index(\"df\",[[field_name]])\n",
    "      py.add_line(py.assign(f1,py.call(\"MinMaxScaler().fit_transform\",f2)))\n",
    "      py.add_line(py.call(\"x_fields.append\",py.str(field_name)))\n",
    "    elif field[FIELD_ACTION] == FIELD_ACTION_DUMMY:\n",
    "      py.add_line(py.assign(\"dummies\", \n",
    "            py.call(\"pd.get_dummies\",\n",
    "            py.index('df',[py.str(field_name)]),\n",
    "            {'prefix':py.str(field_name),'drop_first':'True'})))\n",
    "      py.add_line(\"df = pd.concat([df,dummies],axis=1)\")\n",
    "      py.add_line(\"x_fields += dummies.columns.tolist()\")\n",
    "      \n",
    "\n",
    "\n",
    "  py.add_line(py.assign(\"x\",py.index(\"df\",[\"x_fields\"],\"values\")))\n",
    "\n",
    "  if metadata[META_TYPE] == META_TYPE_CLASSIFICATION:\n",
    "    py.add_line(py.assign(\"dummies\", py.call(\"pd.get_dummies\", py.index(\"df\", [py.str(target)]))))\n",
    "    py.add_line(py.assign(\"species\", \"dummies.columns\"))\n",
    "    py.add_line(py.assign(\"y\", \"dummies.values\"))\n",
    "  elif metadata[META_TYPE] == META_TYPE_BINARY_CLASSIFICATION:\n",
    "    t = py.index(\"df\",[py.str(target)])\n",
    "    pos = metadata[META_POSITIVE_TOKEN]\n",
    "    py.add_line(py.assign(t,f\"({t}=={py.str(pos)}).astype(int)\"))\n",
    "    py.add_line(py.assign(\"y\", f\"df.{target}.values\"))\n",
    "  else:\n",
    "    py.add_line(py.assign(\"y\", f\"df.{target}.values\"))\n",
    "\n",
    "  py.add_line(py.comment(\"Construct model\"))\n",
    "  # Early stop\n",
    "  if metadata[META_EARLY_STOP]:\n",
    "    x_train, y_train, x_test, y_test = \"x_train\", \"y_train\", \"x_test\", \"y_test\"\n",
    "    py.add_from(\"sklearn.model_selection\", \"train_test_split\")\n",
    "    py.add_line(py.comment(\"Split into validation and training sets\"))\n",
    "    py.add_line(py.assign(f\"{x_train}, {x_test}, {y_train}, {y_test}\",\n",
    "        py.call(\"train_test_split\",\"x\",\"y\",{\"test_size\":0.25,\"random_state\":42})))\n",
    "  else:\n",
    "    x_train, y_train, x_test, y_test = \"x\", \"y\", \"x\", \"y\"\n",
    "\n",
    "  py.add_line(py.assign(\"model\",py.call(\"Sequential\")))\n",
    "  py.add_line(py.call(\"model.add\", py.call(\"Dense\",50,{\"input_dim\":\"x.shape[1]\", \"activation\":py.str('relu')})))\n",
    "  py.add_line(py.call(\"model.add\", py.call(\"Dense\",25,{\"activation\":py.str('relu')})))\n",
    "  if metadata[META_TYPE] == META_TYPE_REGRESSION:\n",
    "    py.add_line(py.call(\"model.add\", py.call(\"Dense\",\"1\")))\n",
    "  elif metadata[META_TYPE] == META_TYPE_BINARY_CLASSIFICATION:\n",
    "    py.add_line(py.call(\"model.add\", py.call(\"Dense\",\"1\",{\"activation\":py.str('sigmoid')})))\n",
    "  else:  \n",
    "    py.add_line(py.call(\"model.add\", py.call(\"Dense\",\"y.shape[1]\",{\"activation\":py.str('softmax')})))\n",
    "  py.add_line(py.call(\"model.compile\", {\"loss\":py.str(loss), \"optimizer\":py.str('adam')}))\n",
    "\n",
    "  py.add_line(py.comment(\"Train model\"))\n",
    "  if metadata[META_EARLY_STOP]:\n",
    "    py.add_line(py.assign(\"monitor\",py.call(\"EarlyStopping\",{\"monitor\":py.str('val_loss'), \"min_delta\":\"1e-3\", \"patience\":5, \n",
    "        \"verbose\":1, \"mode\":py.str('auto'), \"restore_best_weights\":True})))\n",
    "    py.add_line(py.call(\"model.fit\", x_train, y_train, \n",
    "        {\"validation_data\": f\"({x_test},{y_test})\", 'callbacks':'[monitor]', 'verbose':'2','epochs':1000}))\n",
    "  else:\n",
    "    py.add_line(py.call(\"model.fit\", x_train, y_train, {'verbose':'2','epochs':100}))\n",
    "  \n",
    "  py.add_line(py.comment(\"Evaluate model\"))\n",
    "  py.add_from(\"sklearn\", \"metrics\")\n",
    "  py.add_line(py.assign(\"pred\", py.call(\"model.predict\", x_test)))\n",
    "  if metadata[META_TYPE] == META_TYPE_REGRESSION:\n",
    "    py.add_line(py.comment(\"Measure RMSE error.  RMSE is common for regression.\"))\n",
    "    py.add_line(py.assign(\"score\", py.call(\"np.sqrt\", py.call(\"metrics.mean_squared_error\", \"pred\", y_test))))\n",
    "    py.add_line(\"print(f\\\"Root mean square (RMSE): {score}\\\")\")\n",
    "  if metadata[META_TYPE] == META_TYPE_CLASSIFICATION:\n",
    "    py.add_line(py.assign(\"predict_classes\", py.call(\"np.argmax\", \"pred\", {\"axis\":1})))\n",
    "    py.add_line(py.assign(\"expected_classes\", py.call(\"np.argmax\", y_test, {\"axis\":1})))\n",
    "    py.add_line(py.assign(\"correct\", py.call(\"accuracy_score\", \"expected_classes\", \"predict_classes\")))\n",
    "    py.add_line(\"print(f\\\"Accuracy: {correct}\\\")\")\n",
    "  elif metadata[META_TYPE] == META_TYPE_BINARY_CLASSIFICATION :\n",
    "    py.add_line(py.assign(\"predict_classes\", py.call(\"np.argmax\", \"pred\", {\"axis\":1})))\n",
    "    py.add_line(py.assign(\"correct\", py.call(\"accuracy_score\", y_test, \"predict_classes\")))\n",
    "    py.add_line(\"print(f\\\"Accuracy: {correct}\\\")\")\n",
    "    py.add_line(py.assign(\"fpr, tpr, thresholds\", py.call(\"metrics.roc_curve\", y_test, \"pred\", {\"pos_label\":1})))\n",
    "    py.add_line(py.assign(\"score\",py.call(\"metrics.auc\", \"fpr\", \"tpr\")))\n",
    "    py.add_line(\"print(f\\\"Area Under Curve: {score}\\\")\")\n",
    "  if metadata[META_TYPE] == META_TYPE_CLASSIFICATION or metadata[META_TYPE] == META_TYPE_BINARY_CLASSIFICATION :\n",
    "    py.add_from(\"sklearn.metrics\", \"accuracy_score\")\n",
    "    py.add_line(py.assign(\"score\", py.call(\"metrics.log_loss\", y_test, \"pred\", {'eps': 1e-7})))\n",
    "    py.add_line(\"print(f\\\"Log loss: {score}\\\")\")\n",
    "\n",
    "  return py.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EbDxHCJoR-a5"
   },
   "source": [
    "# Running the AutoML Generator\n",
    "\n",
    "We begin by analyzing the dataset specified at the top of this notebook. We display the summary statustics on the dataset. You can change the \"action\" for any of these objects if you do not like the preprocessing action detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 589
    },
    "id": "_mQ_Ovee_1NM",
    "outputId": "3873b101-50b2-4e91-e1a0-2470b57c4fac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fields': {'a1': {'type': 'str', 'mode': 'b', 'action': 'dummy', 'missing': 12, 'unique': 3}, 'a2': {'type': 'float', 'median': 28.46, 'mean': 31.56817109144543, 'sd': 11.957862498270877, 'max': 80.25, 'min': 13.75, 'shapiro-stat': nan, 'shapiro-p': 1.0, 'action': 'zscore', 'missing': 12, 'unique': 350}, 's3': {'type': 'float', 'median': 2.75, 'mean': 4.758724637681159, 'sd': 4.978163248528541, 'max': 28.0, 'min': 0.0, 'shapiro-stat': 0.8302544355392456, 'shapiro-p': 2.0092734071726269e-26, 'action': 'normalize', 'missing': 0, 'unique': 215}, 'a4': {'type': 'str', 'mode': 'u', 'action': 'dummy', 'missing': 6, 'unique': 4}, 'a5': {'type': 'str', 'mode': 'g', 'action': 'dummy', 'missing': 6, 'unique': 4}, 'a6': {'type': 'str', 'mode': 'c', 'action': 'dummy', 'missing': 9, 'unique': 15}, 'a7': {'type': 'str', 'mode': 'v', 'action': 'dummy', 'missing': 9, 'unique': 10}, 'a8': {'type': 'float', 'median': 1.0, 'mean': 2.223405797101449, 'sd': 3.3465133592781324, 'max': 28.5, 'min': 0.0, 'shapiro-stat': 0.665294885635376, 'shapiro-p': 1.109698924394265e-34, 'action': 'normalize', 'missing': 0, 'unique': 132}, 'a9': {'type': 'str', 'mode': 't', 'action': 'dummy', 'missing': 0, 'unique': 2}, 'a10': {'type': 'str', 'mode': 'f', 'action': 'dummy', 'missing': 0, 'unique': 2}, 'a11': {'type': 'int', 'median': 0.0, 'mean': 2.4, 'sd': 4.862940034226996, 'max': 67, 'min': 0, 'shapiro-stat': 0.5330631732940674, 'shapiro-p': 3.662308950796017e-39, 'action': 'normalize', 'missing': 0, 'unique': 23}, 'a12': {'type': 'str', 'mode': 'f', 'action': 'dummy', 'missing': 0, 'unique': 2}, 'a13': {'type': 'str', 'mode': 'g', 'action': 'dummy', 'missing': 0, 'unique': 3}, 'a14': {'type': 'float', 'median': 160.0, 'mean': 184.01477104874445, 'sd': 173.80676822523813, 'max': 2000.0, 'min': 0.0, 'shapiro-stat': nan, 'shapiro-p': 1.0, 'action': 'zscore', 'missing': 13, 'unique': 171}, 'a15': {'type': 'int', 'median': 5.0, 'mean': 1017.3855072463768, 'sd': 5210.10259830269, 'max': 100000, 'min': 0, 'shapiro-stat': 0.16985440254211426, 'shapiro-p': 0.0, 'action': 'normalize', 'missing': 0, 'unique': 240}, 'a16': {'type': 'str', 'mode': '-', 'action': 'target', 'missing': 0, 'unique': 2}}, 'target': 'a16', 'source': 'https://data.heatonresearch.com/data/t81-558/crx.csv', 'early-stop': True, 'type': 'binary-classification', 'positive-token': '+'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-1d74965c-c03b-4963-9034-010fbc99aaae\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>median</th>\n",
       "      <th>mode</th>\n",
       "      <th>max</th>\n",
       "      <th>action</th>\n",
       "      <th>unique</th>\n",
       "      <th>shapiro-p</th>\n",
       "      <th>missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dummy</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a2</td>\n",
       "      <td>31.568171</td>\n",
       "      <td>11.957862</td>\n",
       "      <td>28.46</td>\n",
       "      <td>None</td>\n",
       "      <td>80.25</td>\n",
       "      <td>zscore</td>\n",
       "      <td>350</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s3</td>\n",
       "      <td>4.758725</td>\n",
       "      <td>4.978163</td>\n",
       "      <td>2.75</td>\n",
       "      <td>None</td>\n",
       "      <td>28.00</td>\n",
       "      <td>normalize</td>\n",
       "      <td>215</td>\n",
       "      <td>2.009273e-26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dummy</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>g</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dummy</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>a6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dummy</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>a7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>v</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dummy</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a8</td>\n",
       "      <td>2.223406</td>\n",
       "      <td>3.346513</td>\n",
       "      <td>1.00</td>\n",
       "      <td>None</td>\n",
       "      <td>28.50</td>\n",
       "      <td>normalize</td>\n",
       "      <td>132</td>\n",
       "      <td>1.109699e-34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>a9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dummy</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>a10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dummy</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>a11</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>4.862940</td>\n",
       "      <td>0.00</td>\n",
       "      <td>None</td>\n",
       "      <td>67.00</td>\n",
       "      <td>normalize</td>\n",
       "      <td>23</td>\n",
       "      <td>3.662309e-39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>a12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dummy</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>a13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>g</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dummy</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>a14</td>\n",
       "      <td>184.014771</td>\n",
       "      <td>173.806768</td>\n",
       "      <td>160.00</td>\n",
       "      <td>None</td>\n",
       "      <td>2000.00</td>\n",
       "      <td>zscore</td>\n",
       "      <td>171</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>a15</td>\n",
       "      <td>1017.385507</td>\n",
       "      <td>5210.102598</td>\n",
       "      <td>5.00</td>\n",
       "      <td>None</td>\n",
       "      <td>100000.00</td>\n",
       "      <td>normalize</td>\n",
       "      <td>240</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>a16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>target</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1d74965c-c03b-4963-9034-010fbc99aaae')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-1d74965c-c03b-4963-9034-010fbc99aaae button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-1d74965c-c03b-4963-9034-010fbc99aaae');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   name         mean           sd  median  mode        max     action  unique  \\\n",
       "0    a1          NaN          NaN     NaN     b        NaN      dummy       3   \n",
       "1    a2    31.568171    11.957862   28.46  None      80.25     zscore     350   \n",
       "2    s3     4.758725     4.978163    2.75  None      28.00  normalize     215   \n",
       "3    a4          NaN          NaN     NaN     u        NaN      dummy       4   \n",
       "4    a5          NaN          NaN     NaN     g        NaN      dummy       4   \n",
       "5    a6          NaN          NaN     NaN     c        NaN      dummy      15   \n",
       "6    a7          NaN          NaN     NaN     v        NaN      dummy      10   \n",
       "7    a8     2.223406     3.346513    1.00  None      28.50  normalize     132   \n",
       "8    a9          NaN          NaN     NaN     t        NaN      dummy       2   \n",
       "9   a10          NaN          NaN     NaN     f        NaN      dummy       2   \n",
       "10  a11     2.400000     4.862940    0.00  None      67.00  normalize      23   \n",
       "11  a12          NaN          NaN     NaN     f        NaN      dummy       2   \n",
       "12  a13          NaN          NaN     NaN     g        NaN      dummy       3   \n",
       "13  a14   184.014771   173.806768  160.00  None    2000.00     zscore     171   \n",
       "14  a15  1017.385507  5210.102598    5.00  None  100000.00  normalize     240   \n",
       "15  a16          NaN          NaN     NaN     -        NaN     target       2   \n",
       "\n",
       "       shapiro-p  missing  \n",
       "0            NaN       12  \n",
       "1   1.000000e+00       12  \n",
       "2   2.009273e-26        0  \n",
       "3            NaN        6  \n",
       "4            NaN        6  \n",
       "5            NaN        9  \n",
       "6            NaN        9  \n",
       "7   1.109699e-34        0  \n",
       "8            NaN        0  \n",
       "9            NaN        0  \n",
       "10  3.662309e-39        0  \n",
       "11           NaN        0  \n",
       "12           NaN        0  \n",
       "13  1.000000e+00       13  \n",
       "14  0.000000e+00        0  \n",
       "15           NaN        0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metadata = analyze(DATA_SOURCE, TARGET, IS_REGRESSION)\n",
    "print(metadata)\n",
    "summary = field_summary(metadata)\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsbX5T3ISEpQ"
   },
   "source": [
    "Next we generate the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pCjxl-tkABRk",
    "outputId": "baa280ea-8d32-46db-f910-2f91ff2010ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\n",
      "import io\n",
      "import requests\n",
      "import numpy as np\n",
      "from tensorflow.keras.models import Sequential\n",
      "from tensorflow.keras.layers import Dense, Activation\n",
      "from tensorflow.keras.callbacks import EarlyStopping\n",
      "from scipy.stats import zscore\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "from sklearn import metrics\n",
      "from sklearn.metrics import accuracy_score\n",
      "df = pd.read_csv(\"https://data.heatonresearch.com/data/t81-558/crx.csv\", na_values=['NA', '?'])\n",
      "x_fields = []\n",
      "df[\"a1\"] = df[\"a1\"].fillna(df[\"a1\"].mode()[0])\n",
      "dummies = pd.get_dummies(df[\"a1\"], prefix=\"a1\", drop_first=True)\n",
      "df = pd.concat([df,dummies],axis=1)\n",
      "x_fields += dummies.columns.tolist()\n",
      "df[\"a2\"] = df[\"a2\"].fillna(df[\"a2\"].median())\n",
      "df[\"a2\"] = zscore(df[\"a2\"])\n",
      "x_fields.append(\"a2\")\n",
      "df[\"s3\"] = MinMaxScaler().fit_transform(df[['s3']])\n",
      "x_fields.append(\"s3\")\n",
      "df[\"a4\"] = df[\"a4\"].fillna(df[\"a4\"].mode()[0])\n",
      "dummies = pd.get_dummies(df[\"a4\"], prefix=\"a4\", drop_first=True)\n",
      "df = pd.concat([df,dummies],axis=1)\n",
      "x_fields += dummies.columns.tolist()\n",
      "df[\"a5\"] = df[\"a5\"].fillna(df[\"a5\"].mode()[0])\n",
      "dummies = pd.get_dummies(df[\"a5\"], prefix=\"a5\", drop_first=True)\n",
      "df = pd.concat([df,dummies],axis=1)\n",
      "x_fields += dummies.columns.tolist()\n",
      "df[\"a6\"] = df[\"a6\"].fillna(df[\"a6\"].mode()[0])\n",
      "dummies = pd.get_dummies(df[\"a6\"], prefix=\"a6\", drop_first=True)\n",
      "df = pd.concat([df,dummies],axis=1)\n",
      "x_fields += dummies.columns.tolist()\n",
      "df[\"a7\"] = df[\"a7\"].fillna(df[\"a7\"].mode()[0])\n",
      "dummies = pd.get_dummies(df[\"a7\"], prefix=\"a7\", drop_first=True)\n",
      "df = pd.concat([df,dummies],axis=1)\n",
      "x_fields += dummies.columns.tolist()\n",
      "df[\"a8\"] = MinMaxScaler().fit_transform(df[['a8']])\n",
      "x_fields.append(\"a8\")\n",
      "dummies = pd.get_dummies(df[\"a9\"], prefix=\"a9\", drop_first=True)\n",
      "df = pd.concat([df,dummies],axis=1)\n",
      "x_fields += dummies.columns.tolist()\n",
      "dummies = pd.get_dummies(df[\"a10\"], prefix=\"a10\", drop_first=True)\n",
      "df = pd.concat([df,dummies],axis=1)\n",
      "x_fields += dummies.columns.tolist()\n",
      "df[\"a11\"] = MinMaxScaler().fit_transform(df[['a11']])\n",
      "x_fields.append(\"a11\")\n",
      "dummies = pd.get_dummies(df[\"a12\"], prefix=\"a12\", drop_first=True)\n",
      "df = pd.concat([df,dummies],axis=1)\n",
      "x_fields += dummies.columns.tolist()\n",
      "dummies = pd.get_dummies(df[\"a13\"], prefix=\"a13\", drop_first=True)\n",
      "df = pd.concat([df,dummies],axis=1)\n",
      "x_fields += dummies.columns.tolist()\n",
      "df[\"a14\"] = df[\"a14\"].fillna(df[\"a14\"].median())\n",
      "df[\"a14\"] = zscore(df[\"a14\"])\n",
      "x_fields.append(\"a14\")\n",
      "df[\"a15\"] = MinMaxScaler().fit_transform(df[['a15']])\n",
      "x_fields.append(\"a15\")\n",
      "x = df[x_fields].values\n",
      "df[\"a16\"] = (df[\"a16\"]==\"+\").astype(int)\n",
      "y = df.a16.values\n",
      "# Construct model\n",
      "model = Sequential()\n",
      "model.add(Dense(50, input_dim=x.shape[1], activation=\"relu\"))\n",
      "model.add(Dense(25, activation=\"relu\"))\n",
      "model.add(Dense(1, activation=\"sigmoid\"))\n",
      "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
      "# Train model\n",
      "model.fit(x, y, verbose=2, epochs=100)\n",
      "# Evaluate model\n",
      "pred = model.predict(x)\n",
      "predict_classes = np.argmax(pred, axis=1)\n",
      "correct = accuracy_score(y, predict_classes)\n",
      "print(f\"Accuracy: {correct}\")\n",
      "fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=1)\n",
      "score = metrics.auc(fpr, tpr)\n",
      "print(f\"Area Under Curve: {score}\")\n",
      "score = metrics.log_loss(y, pred, eps=1e-07)\n",
      "print(f\"Log loss: {score}\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metadata[META_EARLY_STOP] = False\n",
    "python_code = generate_keras(metadata)\n",
    "print(python_code)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
