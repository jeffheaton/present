{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9V_9RChmQGs7"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeffheaton/present/blob/master/youtube/automl/simple-automl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "\n",
    "# Simple AutoML\n",
    "\n",
    "Copyright 2023 by [Jeff Heaton](https://youtube.com/@HeatonResearch), LGPL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "htVks4vk_BPL"
   },
   "outputs": [],
   "source": [
    "#DATA_SOURCE = \"https://data.heatonresearch.com/data/t81-558/auto-mpg.csv\"; TARGET = \"mpg\";IS_REGRESSION=True\n",
    "#DATA_SOURCE = \"https://data.heatonresearch.com/data/t81-558/iris.csv\"; TARGET = \"species\";IS_REGRESSION=False\n",
    "DATA_SOURCE = \"https://data.heatonresearch.com/data/t81-558/crx.csv\"; TARGET = 'a16'; IS_REGRESSION=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z21pOD98_kQc"
   },
   "source": [
    "# Analyze Data\n",
    "\n",
    "The following code implements the ```analyze``` function that determines the types and encodings of all columns in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DSCFz5EBU4DN"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import shapiro\n",
    "import pandas as pd\n",
    "\n",
    "CONFIG_MAX_DUMMY = \"max_dummy\"\n",
    "CONFIG_MAX_DUMMY_PCT = \"pct_dummy\"\n",
    "\n",
    "CONFIG = {\n",
    "  CONFIG_MAX_DUMMY: 1000,\n",
    "  CONFIG_MAX_DUMMY_PCT: 0.75\n",
    "}\n",
    "\n",
    "def isnumeric(datatype):\n",
    "  return datatype in [FIELD_TYPE_FLOAT,FIELD_TYPE_INT]\n",
    "\n",
    "FIELDS = \"fields\"\n",
    "FIELD_ACTION = \"action\"\n",
    "FIELD_ACTION_COPY = \"copy\"\n",
    "FIELD_ACTION_IGNORE = \"ignore\"\n",
    "FIELD_ACTION_ZSCORE = \"zscore\"\n",
    "FIELD_ACTION_NORMALIZE = \"normalize\"\n",
    "FIELD_ACTION_DUMMY = \"dummy\"\n",
    "FIELD_ACTION_TARGET = \"target\"\n",
    "FIELD_NAME = \"name\"\n",
    "FIELD_SUM = \"sum\"\n",
    "FIELD_TYPE = \"type\"\n",
    "FIELD_MEAN = \"mean\"\n",
    "FIELD_NUM = \"n\"\n",
    "FIELD_MISSING = \"missing\"\n",
    "FIELD_MIN = \"min\"\n",
    "FIELD_MAX = \"max\"\n",
    "FIELD_VAR = \"var\"\n",
    "FIELD_SD = \"sd\"\n",
    "FIELD_UNIQUE = \"unique\"\n",
    "FIELD_MEDIAN = \"median\"\n",
    "FIELD_MODE = \"mode\"\n",
    "FIELD_SHAPIRO_STAT = \"shapiro-stat\"\n",
    "FIELD_SHAPIRO_P = \"shapiro-p\"\n",
    "META_TARGET = \"target\"\n",
    "META_TYPE = \"type\"\n",
    "META_TYPE_BINARY_CLASSIFICATION = \"binary-classification\"\n",
    "META_TYPE_CLASSIFICATION = \"classification\"\n",
    "META_TYPE_REGRESSION = \"regression\"\n",
    "META_SOURCE = \"source\"\n",
    "META_POSITIVE_TOKEN = \"positive-token\"\n",
    "META_EARLY_STOP = \"early-stop\"\n",
    "\n",
    "FIELD_TYPE_FLOAT = \"float\"\n",
    "FIELD_TYPE_INT = \"int\"\n",
    "FIELD_TYPE_STR = \"str\"\n",
    "\n",
    "def find_positive(s):\n",
    "  s = set(s.str.upper().tolist())\n",
    "  if len(s) != 2: return None\n",
    "  if \"+\" in s and \"-\" in s: return \"+\"\n",
    "  if \"0\" in s and \"1\" in s: return \"1\"\n",
    "  if \"t\" in s and \"f\" in s: return \"t\"\n",
    "  if \"y\" in s and \"n\" in s: return \"y\"\n",
    "  if \"true\" in s and \"false\" in s: return \"true\"\n",
    "  if \"yes\" in s and \"no\" in s: return \"yes\"\n",
    "  if \"p\" in s and \"n\" in s: return \"p\"\n",
    "  if \"positive\" in s and \"negative\" in s: return \"positive\"\n",
    "  s = list(s)\n",
    "  s.sort()\n",
    "  return s[0]\n",
    "\n",
    "def analyze(data_source, target, is_regression=True):\n",
    "  df = pd.read_csv(data_source,na_values=['NA', '?'])\n",
    "\n",
    "  metadata = {\n",
    "      FIELDS: {},\n",
    "      META_TARGET: target,\n",
    "      META_SOURCE: data_source,\n",
    "      META_EARLY_STOP: True\n",
    "  }\n",
    "\n",
    "  fields = metadata[FIELDS]\n",
    "\n",
    "  for field_name,csv_type in zip(df.columns,df.dtypes):\n",
    "    #print(name,csv_type)\n",
    "    if \"float\" in csv_type.name:\n",
    "      dtype = FIELD_TYPE_FLOAT\n",
    "      action = FIELD_ACTION_COPY\n",
    "    elif \"int\" in csv_type.name:\n",
    "      dtype = FIELD_TYPE_INT\n",
    "      action = FIELD_ACTION_COPY\n",
    "    else:\n",
    "      dtype = FIELD_TYPE_STR\n",
    "      action = FIELD_ACTION_IGNORE\n",
    "\n",
    "    missing_count = sum(df[field_name].isnull())\n",
    "    col = df[field_name]\n",
    "    unique_count = len(pd.unique(col))\n",
    "\n",
    "    if isnumeric(dtype):\n",
    "      stat, p = shapiro(col)\n",
    "\n",
    "      # less than or equal to 0.05 not normal\n",
    "      action = FIELD_ACTION_ZSCORE if p>0.05 else FIELD_ACTION_NORMALIZE\n",
    "\n",
    "      fields[field_name] = {\n",
    "          FIELD_TYPE:dtype,\n",
    "          FIELD_MEDIAN:col.median(),\n",
    "          FIELD_MEAN:col.mean(),\n",
    "          FIELD_SD:col.std(),\n",
    "          FIELD_MAX:col.max(),\n",
    "          FIELD_MIN:col.min(),\n",
    "          FIELD_SHAPIRO_STAT:stat,\n",
    "          FIELD_SHAPIRO_P:p,\n",
    "          FIELD_ACTION:action,\n",
    "          FIELD_MISSING:missing_count,\n",
    "          FIELD_UNIQUE:unique_count}\n",
    "\n",
    "    else:\n",
    "      fields[field_name] = {\n",
    "          FIELD_TYPE:dtype,\n",
    "          FIELD_MODE:col.mode()[0],\n",
    "          FIELD_ACTION:action,\n",
    "          FIELD_MISSING:missing_count,\n",
    "          FIELD_UNIQUE:unique_count}\n",
    "\n",
    "    # Determine action\n",
    "    field = fields[field_name]\n",
    "    if (field[FIELD_TYPE] == FIELD_TYPE_STR) and (field[FIELD_UNIQUE]<CONFIG[CONFIG_MAX_DUMMY]) and (field[FIELD_UNIQUE]/len(df)<CONFIG[CONFIG_MAX_DUMMY_PCT]):\n",
    "      field[FIELD_ACTION] = FIELD_ACTION_DUMMY\n",
    "    if field_name == target:\n",
    "      field[FIELD_ACTION] = FIELD_ACTION_TARGET\n",
    "  \n",
    "  # Determine model type\n",
    "  is_binary = (metadata[FIELDS][target][FIELD_UNIQUE]==2) and not is_regression\n",
    "\n",
    "  if is_regression:\n",
    "    metadata[META_TYPE] = META_TYPE_REGRESSION\n",
    "  else:\n",
    "    if metadata[FIELDS][target][FIELD_UNIQUE]==2:\n",
    "      metadata[META_TYPE] = META_TYPE_BINARY_CLASSIFICATION\n",
    "\n",
    "      metadata[META_POSITIVE_TOKEN] = find_positive(df[target])\n",
    "    else:\n",
    "      metadata[META_TYPE] = META_TYPE_CLASSIFICATION\n",
    "\n",
    "  return metadata\n",
    "\n",
    "COLS = [FIELD_MEAN, FIELD_SD, FIELD_MEDIAN, FIELD_MODE, FIELD_MAX, FIELD_ACTION, FIELD_UNIQUE, FIELD_SHAPIRO_P,FIELD_MISSING]\n",
    "\n",
    "def field_summary(metadata, cols=COLS):\n",
    "  data = {}\n",
    "\n",
    "  data['name'] = []\n",
    "  for col in cols:\n",
    "    data[col] = []\n",
    "\n",
    "  for field_name in metadata[FIELDS]:\n",
    "    field = metadata[FIELDS][field_name]\n",
    "    data['name'].append(field_name)\n",
    "    for col in cols:\n",
    "      data[col].append(field.get(col, None))\n",
    "\n",
    "  return pd.DataFrame(data)[['name']+COLS]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZOJ_jcYE234"
   },
   "source": [
    "# Generate Code\n",
    "\n",
    "The following code generates Keras Python code for the data that was analzyed in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U3p2kabMhX1G"
   },
   "outputs": [],
   "source": [
    "from dataclasses import MISSING\n",
    "from pandas.core.dtypes.inference import is_re\n",
    "def tolist(obj):\n",
    "  if isinstance(obj,list) or isinstance(obj, tuple):\n",
    "    return obj\n",
    "  else:\n",
    "    return [obj]\n",
    "\n",
    "class PythonFile:\n",
    "  def __init__(self):\n",
    "    self.imports = []\n",
    "    self.lines = []\n",
    "\n",
    "  def add_import(self, name, alias=None):\n",
    "    if alias:\n",
    "      self.imports.append({\"name\": name, \"alias\": alias})\n",
    "    else:\n",
    "      self.imports.append({\"name\": name})\n",
    "\n",
    "  def add_from(self, _from, _import):\n",
    "    self.imports.append({\"from\": _from, \"import\": _import})\n",
    "    \n",
    "  def generate(self):\n",
    "    src = \"\"\n",
    "    for obj in self.imports:\n",
    "      if \"name\" in obj and \"alias\" in obj:\n",
    "        src += f\"import {obj['name']} as {obj['alias']}\"\n",
    "      elif \"name\" in obj and \"alias\" not in obj:\n",
    "        src += f\"import {obj['name']}\"\n",
    "      elif \"from\" in obj and \"import\" in obj:\n",
    "        imports = \", \".join(tolist(obj['import']))\n",
    "        src += f\"from {obj['from']} import {imports}\"\n",
    "\n",
    "      src+=\"\\n\"\n",
    "\n",
    "    for line in self.lines:\n",
    "      src+=line+\"\\n\"\n",
    "    return src\n",
    "\n",
    "  def add_line(self, str):\n",
    "    self.lines.append(str)\n",
    "\n",
    "  def comment(self, str):\n",
    "    return f\"# {str}\"\n",
    "\n",
    "  def call(self, name, *args):\n",
    "    src = name + \"(\"\n",
    "\n",
    "    formatted_args = []\n",
    "    started_named = False\n",
    "    for arg in args:\n",
    "      if isinstance(arg,dict):\n",
    "        formatted_args += [f\"{name}={arg[name]}\" for name in arg.keys()]\n",
    "        started_named = True\n",
    "      else: \n",
    "        if started_named: raise ValueError(\"positional argument follows keyword argument\")\n",
    "        formatted_args.append(str(arg))\n",
    "\n",
    "    src += \", \".join(formatted_args)\n",
    "    src += \")\"\n",
    "    return src\n",
    "\n",
    "  def assign(self, left, right):\n",
    "    return f\"{left} = {right}\"\n",
    "\n",
    "  def str(self, str):\n",
    "    return f\"\\\"{str}\\\"\"\n",
    "\n",
    "  def index(self, name, indexes, dot=None):\n",
    "    src = name\n",
    "    for idx in indexes:\n",
    "      src+=f'[{idx}]'\n",
    "\n",
    "    if dot:\n",
    "      src+='.'\n",
    "      src+=dot\n",
    "    return src\n",
    "    \n",
    "def generate_keras(metadata):\n",
    "  na_values = ['NA', '?']\n",
    "  target = metadata[META_TARGET]\n",
    "  is_regression = metadata[META_TYPE] == META_TYPE_REGRESSION\n",
    "  is_binary = (metadata[FIELDS][target][FIELD_UNIQUE]==2) and (metadata[META_TYPE]==META_TYPE_CLASSIFICATION)\n",
    "\n",
    "  if metadata[META_TYPE] == META_TYPE_REGRESSION:\n",
    "    loss = \"mean_squared_error\"\n",
    "  elif metadata[META_TYPE] == META_TYPE_BINARY_CLASSIFICATION:\n",
    "    loss = \"binary_crossentropy\"\n",
    "  else:\n",
    "    loss = \"categorical_crossentropy\"\n",
    "\n",
    "  py = PythonFile()\n",
    "  # Imports\n",
    "  py.add_import(\"pandas\", \"pd\")\n",
    "  py.add_import(\"io\")\n",
    "  py.add_import(\"requests\")\n",
    "  py.add_import(\"numpy\", \"np\")\n",
    "  py.add_from(\"tensorflow.keras.models\", \"Sequential\")\n",
    "  py.add_from(\"tensorflow.keras.layers\", [\"Dense\", \"Activation\"])\n",
    "  py.add_from(\"tensorflow.keras.callbacks\", \"EarlyStopping\")\n",
    "  py.add_from(\"scipy.stats\", \"zscore\")\n",
    "  py.add_from(\"sklearn.preprocessing\", \"MinMaxScaler\")\n",
    "\n",
    "  py.add_line(py.assign(\"df\", py.call(\"pd.read_csv\",py.str(metadata[META_SOURCE]),{'na_values':na_values})))\n",
    "  x_fields = [x for x in metadata[FIELDS] if x != target and metadata[FIELDS][x][FIELD_ACTION] in [FIELD_ACTION_COPY]]\n",
    "  py.add_line(py.assign(\"x_fields\",x_fields))\n",
    "  \n",
    "  # Analyze input columns\n",
    "  for field_name in metadata[FIELDS]:\n",
    "    field = metadata[FIELDS][field_name]\n",
    "    if field[FIELD_MISSING]>0:\n",
    "      if isnumeric(field[FIELD_TYPE]):\n",
    "        fn = \"median\"\n",
    "        suffix = \"\"\n",
    "      else:\n",
    "        fn = \"mode\"\n",
    "        suffix = \"[0]\"\n",
    "      py.add_line(py.assign(py.index(\"df\",[py.str(field_name)]),\n",
    "                py.index(\"df\",[py.str(field_name)],py.call(\"fillna\",\n",
    "                py.index(\"df\",[py.str(field_name)],py.call(fn)+suffix)\n",
    "                ))))\n",
    "    if field[FIELD_ACTION] == FIELD_ACTION_ZSCORE:\n",
    "      py.add_line(py.assign(py.index(\"df\",[py.str(field_name)]),\n",
    "                py.call(\"zscore\",py.index(\"df\",[py.str(field_name)]))))\n",
    "      py.add_line(py.call(\"x_fields.append\",py.str(field_name)))\n",
    "    elif field[FIELD_ACTION] == FIELD_ACTION_NORMALIZE:\n",
    "      f1 = py.index(\"df\",[py.str(field_name)])\n",
    "      f2 = py.index(\"df\",[[field_name]])\n",
    "      py.add_line(py.assign(f1,py.call(\"MinMaxScaler().fit_transform\",f2)))\n",
    "      py.add_line(py.call(\"x_fields.append\",py.str(field_name)))\n",
    "    elif field[FIELD_ACTION] == FIELD_ACTION_DUMMY:\n",
    "      py.add_line(py.assign(\"dummies\", \n",
    "            py.call(\"pd.get_dummies\",\n",
    "            py.index('df',[py.str(field_name)]),\n",
    "            {'prefix':py.str(field_name),'drop_first':'True'})))\n",
    "      py.add_line(\"df = pd.concat([df,dummies],axis=1)\")\n",
    "      py.add_line(\"x_fields += dummies.columns.tolist()\")\n",
    "      \n",
    "\n",
    "\n",
    "  py.add_line(py.assign(\"x\",py.index(\"df\",[\"x_fields\"],\"values\")))\n",
    "\n",
    "  if metadata[META_TYPE] == META_TYPE_CLASSIFICATION:\n",
    "    py.add_line(py.assign(\"dummies\", py.call(\"pd.get_dummies\", py.index(\"df\", [py.str(target)]))))\n",
    "    py.add_line(py.assign(\"species\", \"dummies.columns\"))\n",
    "    py.add_line(py.assign(\"y\", \"dummies.values\"))\n",
    "  elif metadata[META_TYPE] == META_TYPE_BINARY_CLASSIFICATION:\n",
    "    t = py.index(\"df\",[py.str(target)])\n",
    "    pos = metadata[META_POSITIVE_TOKEN]\n",
    "    py.add_line(py.assign(t,f\"({t}=={py.str(pos)}).astype(int)\"))\n",
    "    py.add_line(py.assign(\"y\", f\"df.{target}.values\"))\n",
    "  else:\n",
    "    py.add_line(py.assign(\"y\", f\"df.{target}.values\"))\n",
    "\n",
    "  py.add_line(py.comment(\"Construct model\"))\n",
    "  # Early stop\n",
    "  if metadata[META_EARLY_STOP]:\n",
    "    x_train, y_train, x_test, y_test = \"x_train\", \"y_train\", \"x_test\", \"y_test\"\n",
    "    py.add_from(\"sklearn.model_selection\", \"train_test_split\")\n",
    "    py.add_line(py.comment(\"Split into validation and training sets\"))\n",
    "    py.add_line(py.assign(f\"{x_train}, {x_test}, {y_train}, {y_test}\",\n",
    "        py.call(\"train_test_split\",\"x\",\"y\",{\"test_size\":0.25,\"random_state\":42})))\n",
    "  else:\n",
    "    x_train, y_train, x_test, y_test = \"x\", \"y\", \"x\", \"y\"\n",
    "\n",
    "  py.add_line(py.assign(\"model\",py.call(\"Sequential\")))\n",
    "  py.add_line(py.call(\"model.add\", py.call(\"Dense\",50,{\"input_dim\":\"x.shape[1]\", \"activation\":py.str('relu')})))\n",
    "  py.add_line(py.call(\"model.add\", py.call(\"Dense\",25,{\"activation\":py.str('relu')})))\n",
    "  if metadata[META_TYPE] == META_TYPE_REGRESSION:\n",
    "    py.add_line(py.call(\"model.add\", py.call(\"Dense\",\"1\")))\n",
    "  elif metadata[META_TYPE] == META_TYPE_BINARY_CLASSIFICATION:\n",
    "    py.add_line(py.call(\"model.add\", py.call(\"Dense\",\"1\",{\"activation\":py.str('sigmoid')})))\n",
    "  else:  \n",
    "    py.add_line(py.call(\"model.add\", py.call(\"Dense\",\"y.shape[1]\",{\"activation\":py.str('softmax')})))\n",
    "  py.add_line(py.call(\"model.compile\", {\"loss\":py.str(loss), \"optimizer\":py.str('adam')}))\n",
    "\n",
    "  py.add_line(py.comment(\"Train model\"))\n",
    "  if metadata[META_EARLY_STOP]:\n",
    "    py.add_line(py.assign(\"monitor\",py.call(\"EarlyStopping\",{\"monitor\":py.str('val_loss'), \"min_delta\":\"1e-3\", \"patience\":5, \n",
    "        \"verbose\":1, \"mode\":py.str('auto'), \"restore_best_weights\":True})))\n",
    "    py.add_line(py.call(\"model.fit\", x_train, y_train, \n",
    "        {\"validation_data\": f\"({x_test},{y_test})\", 'callbacks':'[monitor]', 'verbose':'2','epochs':1000}))\n",
    "  else:\n",
    "    py.add_line(py.call(\"model.fit\", x_train, y_train, {'verbose':'2','epochs':100}))\n",
    "  \n",
    "  py.add_line(py.comment(\"Evaluate model\"))\n",
    "  py.add_from(\"sklearn\", \"metrics\")\n",
    "  py.add_line(py.assign(\"pred\", py.call(\"model.predict\", x_test)))\n",
    "  if metadata[META_TYPE] == META_TYPE_REGRESSION:\n",
    "    py.add_line(py.comment(\"Measure RMSE error.  RMSE is common for regression.\"))\n",
    "    py.add_line(py.assign(\"score\", py.call(\"np.sqrt\", py.call(\"metrics.mean_squared_error\", \"pred\", y_test))))\n",
    "    py.add_line(\"print(f\\\"Root mean square (RMSE): {score}\\\")\")\n",
    "  if metadata[META_TYPE] == META_TYPE_CLASSIFICATION:\n",
    "    py.add_line(py.assign(\"predict_classes\", py.call(\"np.argmax\", \"pred\", {\"axis\":1})))\n",
    "    py.add_line(py.assign(\"expected_classes\", py.call(\"np.argmax\", y_test, {\"axis\":1})))\n",
    "    py.add_line(py.assign(\"correct\", py.call(\"accuracy_score\", \"expected_classes\", \"predict_classes\")))\n",
    "    py.add_line(\"print(f\\\"Accuracy: {correct}\\\")\")\n",
    "  elif metadata[META_TYPE] == META_TYPE_BINARY_CLASSIFICATION :\n",
    "    py.add_line(py.assign(\"predict_classes\", py.call(\"np.argmax\", \"pred\", {\"axis\":1})))\n",
    "    py.add_line(py.assign(\"correct\", py.call(\"accuracy_score\", y_test, \"predict_classes\")))\n",
    "    py.add_line(\"print(f\\\"Accuracy: {correct}\\\")\")\n",
    "    py.add_line(py.assign(\"fpr, tpr, thresholds\", py.call(\"metrics.roc_curve\", y_test, \"pred\", {\"pos_label\":1})))\n",
    "    py.add_line(py.assign(\"score\",py.call(\"metrics.auc\", \"fpr\", \"tpr\")))\n",
    "    py.add_line(\"print(f\\\"Area Under Curve: {score}\\\")\")\n",
    "  if metadata[META_TYPE] == META_TYPE_CLASSIFICATION or metadata[META_TYPE] == META_TYPE_BINARY_CLASSIFICATION :\n",
    "    py.add_from(\"sklearn.metrics\", \"accuracy_score\")\n",
    "    py.add_line(py.assign(\"score\", py.call(\"metrics.log_loss\", y_test, \"pred\", {'eps': 1e-7})))\n",
    "    py.add_line(\"print(f\\\"Log loss: {score}\\\")\")\n",
    "\n",
    "  return py.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EbDxHCJoR-a5"
   },
   "source": [
    "# Running the AutoML Generator\n",
    "\n",
    "We begin by analyzing the dataset specified at the top of this notebook. We display the summary statustics on the dataset. You can change the \"action\" for any of these objects if you do not like the preprocessing action detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 589
    },
    "id": "_mQ_Ovee_1NM",
    "outputId": "3873b101-50b2-4e91-e1a0-2470b57c4fac"
   },
   "outputs": [],
   "source": [
    "metadata = analyze(DATA_SOURCE, TARGET, IS_REGRESSION)\n",
    "print(metadata)\n",
    "summary = field_summary(metadata)\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsbX5T3ISEpQ"
   },
   "source": [
    "Next we generate the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pCjxl-tkABRk",
    "outputId": "baa280ea-8d32-46db-f910-2f91ff2010ce"
   },
   "outputs": [],
   "source": [
    "metadata[META_EARLY_STOP] = False\n",
    "python_code = generate_keras(metadata)\n",
    "print(python_code)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
